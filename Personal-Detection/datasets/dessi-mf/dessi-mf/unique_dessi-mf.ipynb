{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Join mimesis, faker and dessi data to one dataset  \n",
    "- Provide binary labels (personal/non-personal)  \n",
    "- Ensure that classes with unique values are still unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\AppData\\Local\\Temp\\ipykernel_11288\\1651087509.py:8: DtypeWarning: Columns (1855,4493,4607,5938,8673,9994,10789,10922,12402,14133,16114,16224,16975,17584,17907) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mimesis_train = pd.read_csv(\"../mimesis/train.csv\")\n",
      "C:\\Users\\Luca\\AppData\\Local\\Temp\\ipykernel_11288\\1651087509.py:18: DtypeWarning: Columns (673,711,1251,1458,1502,1734,1847,1856,1878,1937,2085,2498,2760,3043,3617,4024,4165,4409,4563,4708,4912,5353,5446,5724,6034,6381,6505,6781,7835,8237,8587,8891,9099,9104,9815,9853,10586,10948,11316,11571,13141,13847,13910,14134,14212,14381,14839,15295,15582,16355,16368,18340,18731) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  faker_train = pd.read_csv(\"../faker/train.csv\")\n"
     ]
    }
   ],
   "source": [
    "dessi_train = pd.read_csv(\"../dessi_unique/train.csv\")\n",
    "dessi_train_labels = pd.read_csv(\"../dessi_unique/train_labels.csv\")\n",
    "dessi_dev = pd.read_csv(\"../dessi_unique/dev.csv\")\n",
    "dessi_dev_labels = pd.read_csv(\"../dessi_unique/dev_labels.csv\")\n",
    "dessi_test = pd.read_csv(\"../dessi_unique/test.csv\")\n",
    "dessi_test_labels = pd.read_csv(\"../dessi_unique/test_labels.csv\")\n",
    "\n",
    "mimesis_train = pd.read_csv(\"../mimesis/train.csv\")\n",
    "mimesis_train_labels = pd.read_csv(\"../mimesis/train_labels.csv\")\n",
    "mimesis_train_classes = pd.read_csv(\"../mimesis/train_classes.csv\")\n",
    "mimesis_dev = pd.read_csv(\"../mimesis/dev.csv\")\n",
    "mimesis_dev_labels = pd.read_csv(\"../mimesis/dev_labels.csv\")\n",
    "mimesis_dev_classes = pd.read_csv(\"../mimesis/dev_classes.csv\")\n",
    "mimesis_test = pd.read_csv(\"../mimesis/test.csv\")\n",
    "mimesis_test_labels = pd.read_csv(\"../mimesis/test_labels.csv\")\n",
    "mimesis_test_classes = pd.read_csv(\"../mimesis/test_classes.csv\")\n",
    "\n",
    "faker_train = pd.read_csv(\"../faker/train.csv\")\n",
    "faker_train_labels = pd.read_csv(\"../faker/train_labels.csv\")\n",
    "faker_train_classes = pd.read_csv(\"../faker/train_classes.csv\")\n",
    "faker_dev = pd.read_csv(\"../faker/dev.csv\")\n",
    "faker_dev_labels = pd.read_csv(\"../faker/dev_labels.csv\")\n",
    "faker_dev_classes = pd.read_csv(\"../faker/dev_classes.csv\")\n",
    "faker_test = pd.read_csv(\"../faker/test.csv\")\n",
    "faker_test_labels = pd.read_csv(\"../faker/test_labels.csv\")\n",
    "faker_test_classes = pd.read_csv(\"../faker/test_classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dessi_labels = pd.concat([dessi_train_labels, dessi_dev_labels, dessi_test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Phone_number',\n",
       " 'Other_data',\n",
       " 'NIN',\n",
       " 'Date',\n",
       " 'Geolocation',\n",
       " 'Gender',\n",
       " 'NIN,Date',\n",
       " 'Phone_number,NIN',\n",
       " 'NIN,Phone_number',\n",
       " 'SWIFT/BIC',\n",
       " 'Date,NIN',\n",
       " 'NIN,Person',\n",
       " 'NIN,Email',\n",
       " 'Person',\n",
       " 'Person,NIN',\n",
       " 'Phone_number,Email',\n",
       " 'Email,Phone_number',\n",
       " 'Email,NIN',\n",
       " 'Passport',\n",
       " 'Organization,Person',\n",
       " 'Person,Organization',\n",
       " 'Person,Email',\n",
       " 'IBAN',\n",
       " 'Email,Person',\n",
       " 'Religion',\n",
       " 'Sexuality',\n",
       " 'Nationality',\n",
       " 'Email,Address',\n",
       " 'Address,Email',\n",
       " 'Address,Geolocation',\n",
       " 'Geolocation,Address',\n",
       " 'CCN',\n",
       " 'ID_Card',\n",
       " 'Race',\n",
       " 'Organization',\n",
       " 'Address',\n",
       " 'GPE',\n",
       " 'Email',\n",
       " 'Address,Phone_number',\n",
       " 'Organization,Phone_number',\n",
       " 'Address,Person,Phone_number',\n",
       " 'Person,Phone_number',\n",
       " 'Organization,Address',\n",
       " 'Address,Person',\n",
       " 'Person,Date']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a for a in all_dessi_labels[\"label\"].value_counts().keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_PERSONAL = [\"GPE\", \"Organization\", \"Date\", \"SWIFT/BIC\"]\n",
    "DROP = [\"Other_data\", \"Address\", \"Person\"]  # \"Person\" is dropped it can contain a full name but also only a surename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Other_data',\n",
       " 'Address',\n",
       " 'Person',\n",
       " 'Person,Email',\n",
       " 'NIN,Person',\n",
       " 'Organization,Person',\n",
       " 'Person,Organization',\n",
       " 'Email,Person',\n",
       " 'Person,NIN',\n",
       " 'Geolocation,Address',\n",
       " 'Email,Address',\n",
       " 'Address,Email',\n",
       " 'Address,Geolocation',\n",
       " 'Address,Person',\n",
       " 'Address,Person',\n",
       " 'Address,Phone_number',\n",
       " 'Address,Person,Phone_number',\n",
       " 'Address,Person,Phone_number',\n",
       " 'Address,Person,Phone_number',\n",
       " 'Address,Person,Phone_number',\n",
       " 'Person,Phone_number',\n",
       " 'Organization,Address',\n",
       " 'Person,Date']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for a in all_dessi_labels[\"label\"].unique():\n",
    "    for d in DROP:\n",
    "        if d in a and d != a:\n",
    "            DROP.append(a)\n",
    "DROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible non-personal:  NIN,Date\n",
      "Possible non-personal:  Date,NIN\n",
      "Possible non-personal:  Organization,Phone_number\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['GPE', 'Organization', 'Date', 'SWIFT/BIC']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for a in all_dessi_labels[\"label\"].unique():\n",
    "    if a not in DROP:\n",
    "        for d in NON_PERSONAL:\n",
    "            if d in a and d != a:\n",
    "                print(\"Possible non-personal: \", a)\n",
    "NON_PERSONAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIN,Date and Date,NIN are personal as they contain NIN, do not append it to NON-PERSONAL list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CCN',\n",
       " 'Date,NIN',\n",
       " 'Email',\n",
       " 'Email,NIN',\n",
       " 'Email,Phone_number',\n",
       " 'Gender',\n",
       " 'Geolocation',\n",
       " 'IBAN',\n",
       " 'ID_Card',\n",
       " 'NIN',\n",
       " 'NIN,Date',\n",
       " 'NIN,Email',\n",
       " 'NIN,Phone_number',\n",
       " 'Nationality',\n",
       " 'Organization,Phone_number',\n",
       " 'Passport',\n",
       " 'Phone_number',\n",
       " 'Phone_number,Email',\n",
       " 'Phone_number,NIN',\n",
       " 'Race',\n",
       " 'Religion',\n",
       " 'Sexuality'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERSONAL = set([a for a in all_dessi_labels[\"label\"].value_counts().keys()]) - set(DROP) - set(NON_PERSONAL)\n",
    "PERSONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dessi(data, labels):\n",
    "    # label columns as personal, non-personal or drop it\n",
    "    dropping = []\n",
    "    classes = []\n",
    "    for i in range(data.shape[1]):\n",
    "        if labels.loc[i].values[0] in DROP:\n",
    "            dropping.append(i)\n",
    "            continue\n",
    "        elif labels.loc[i].values[0] in PERSONAL:\n",
    "            classes.append(labels.loc[i].values[0])\n",
    "            labels.loc[i] = \"personal\"\n",
    "        else:\n",
    "            classes.append(labels.loc[i].values[0])\n",
    "            labels.loc[i] = \"non-personal\"\n",
    "    data = data.iloc[:, [i for i in range(data.shape[1]) if i not in dropping]]\n",
    "    labels.drop(dropping, inplace=True)\n",
    "    labels = labels.reset_index(drop=True)\n",
    "    return data, labels, pd.DataFrame(classes).rename(columns={0: \"class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dessi_train_prepared, dessi_train_labels_prepared, dessi_train_classes_prepared = prepare_dessi(dessi_train.copy(), dessi_train_labels.copy())\n",
    "dessi_dev_prepared, dessi_dev_labels_prepared, dessi_dev_classes_prepared = prepare_dessi(dessi_dev.copy(), dessi_dev_labels.copy())\n",
    "dessi_test_prepared, dessi_test_labels_prepared, dessi_test_classes_prepared = prepare_dessi(dessi_test.copy(), dessi_test_labels.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_split_type(data, split_type):\n",
    "    if data.shape[1] == 1:\n",
    "        split_df = pd.DataFrame(data.shape[0] * [split_type])\n",
    "        split_df.columns = [\"split_type\"]\n",
    "        return pd.concat([data, split_df], axis=1).reset_index(drop=True)\n",
    "    else:\n",
    "        split_df = pd.DataFrame(data.shape[1] * [split_type]).T\n",
    "        split_df.columns = data.columns\n",
    "        return pd.concat([data, split_df]).reset_index(drop=True)\n",
    "    \n",
    "def add_split_type_equal(data):\n",
    "    if data.shape[1] == 1:\n",
    "        split_labels = round(data.shape[0] * 0.6) * [\"train\"] + round(data.shape[0] * 0.2) * [\"dev\"] + round(data.shape[0] * 0.2) * [\"test\"]\n",
    "        if len(split_labels) > data.shape[0]:\n",
    "            split_labels = split_labels[:data.shape[0]]\n",
    "        elif len(split_labels) < data.shape[0]:\n",
    "            split_labels += [\"train\"] * (data.shape[0] - len(split_labels))\n",
    "        split_df = pd.DataFrame(split_labels)\n",
    "        split_df.columns = [\"split_type\"]\n",
    "        return pd.concat([data, split_df], axis=1).reset_index(drop=True)\n",
    "    else:\n",
    "        split_labels = round(data.shape[1] * 0.6) * [\"train\"] + round(data.shape[1] * 0.2) * [\"dev\"] + round(data.shape[1] * 0.2) * [\"test\"]\n",
    "        if len(split_labels) > data.shape[1]:\n",
    "            split_labels = split_labels[:data.shape[1]]\n",
    "        elif len(split_labels) < data.shape[1]:\n",
    "            split_labels += [\"train\"] * (data.shape[1] - len(split_labels))\n",
    "        split_df = pd.DataFrame([split_labels])\n",
    "        split_df.columns = data.columns\n",
    "        return pd.concat([data, split_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data of other datasets with respecting unique values for some classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimesis_check_unique = {# map mimesis attribute to dessi attribute to check uniqueness\n",
    "    \"email\": [\"Email\", \"personal\"], \n",
    "    \"phone_number\": [\"Phone_number\", \"personal\"],\n",
    "    \"credit_card_number\": [\"CCN\", \"personal\"]\n",
    "}\n",
    "faker_check_unique = {# map faker attribute to dessi and mimesis attribute to check uniqueness\n",
    "    \"address\": [None, \"address\", \"personal\"],\n",
    "    \"iban\": [\"IBAN\", None, \"personal\"],\n",
    "    \"swift\": [\"SWIFT/BIC\", None, \"non-personal\"],\n",
    "    \"credit_card_number\": [\"CCN\", \"credit_card_number\", \"personal\"],\n",
    "    \"email\": [\"Email\", \"email\", \"personal\"],\n",
    "    \"name\": [None, \"full_name\", \"personal\"],\n",
    "    \"phone_number\": [\"Phone_number\", \"phone_number\", \"personal\"],\n",
    "    \"ssn\": [\"NIN\", None, \"personal\"],\n",
    "    \"passport_number\": [\"Passport\", None, \"personal\"],\n",
    "    \"current_location\": [\"Geolocation\", None, \"personal\"],    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([add_split_type(dessi_train_prepared, \"train\"), add_split_type(dessi_dev_prepared, \"dev\"), add_split_type(dessi_test_prepared, \"test\")], axis=1)\n",
    "all_labels = pd.concat([add_split_type(dessi_train_labels_prepared, \"train\"), add_split_type(dessi_dev_labels_prepared, \"dev\"), \n",
    "                             add_split_type(dessi_test_labels_prepared, \"test\")]).reset_index(drop=True)\n",
    "all_classes = pd.concat([add_split_type(dessi_train_classes_prepared, \"train\"), add_split_type(dessi_dev_classes_prepared, \"dev\"), \n",
    "                         add_split_type(dessi_test_classes_prepared, \"test\")]).reset_index(drop=True)\n",
    "\n",
    "mimesis_data = pd.concat([add_split_type(mimesis_train, \"train\"), add_split_type(mimesis_dev, \"dev\"), add_split_type(mimesis_test, \"test\")], axis=1)\n",
    "mimesis_labels = pd.concat([add_split_type(mimesis_train_labels, \"train\"), add_split_type(mimesis_dev_labels, \"dev\"), add_split_type(mimesis_test_labels, \"test\")]).reset_index(drop=True)\n",
    "mimesis_classes = pd.concat([add_split_type(mimesis_train_classes, \"train\"), add_split_type(mimesis_dev_classes, \"dev\"), add_split_type(mimesis_test_classes, \"test\")]).reset_index(drop=True)\n",
    "\n",
    "faker_data = pd.concat([add_split_type(faker_train, \"train\"), add_split_type(faker_dev, \"dev\"), add_split_type(faker_test, \"test\")], axis=1)\n",
    "faker_labels = pd.concat([add_split_type(faker_train_labels, \"train\"), add_split_type(faker_dev_labels, \"dev\"), add_split_type(faker_test_labels, \"test\")]).reset_index(drop=True)\n",
    "faker_classes = pd.concat([add_split_type(faker_train_classes, \"train\"), add_split_type(faker_dev_classes, \"dev\"), add_split_type(faker_test_classes, \"test\")]).reset_index(drop=True)\n",
    "\n",
    "dessi_all = pd.concat([dessi_train_prepared, dessi_dev_prepared, dessi_test_prepared], axis=1)\n",
    "dessi_all_classes = pd.concat([dessi_train_classes_prepared, dessi_dev_classes_prepared, dessi_test_classes_prepared]).reset_index(drop=True)\n",
    "mimesis_all = pd.concat([mimesis_train, mimesis_dev, mimesis_test], axis=1)\n",
    "mimesis_all_classes = pd.concat([mimesis_train_classes, mimesis_dev_classes, mimesis_test_classes]).reset_index(drop=True)\n",
    "\n",
    "keep = []\n",
    "for a in range(mimesis_data.shape[1]):\n",
    "    if \"mixed\" in mimesis_classes.iloc[a].values[0]:\n",
    "        val = mimesis_classes.iloc[a].values[0][:-6]\n",
    "    else:\n",
    "        val = mimesis_classes.iloc[a].values[0][:-3]\n",
    "    if val not in mimesis_check_unique.keys():\n",
    "        keep.append(a)\n",
    "all_dataset = pd.concat([pd.DataFrame(all_data.shape[1] * [\"dessi\"]), pd.DataFrame(mimesis_data.iloc[:, keep].shape[1] * [\"mimesis\"])]).reset_index(drop=True).rename(columns={0: \"dataset\"})\n",
    "all_data = pd.concat([all_data, mimesis_data.iloc[:, keep]], axis=1)\n",
    "all_labels = pd.concat([all_labels, mimesis_labels.iloc[keep]]).reset_index(drop=True)\n",
    "all_classes = pd.concat([all_classes, mimesis_classes.iloc[keep]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = []\n",
    "for a in range(faker_data.shape[1]):\n",
    "    if \"mixed\" in faker_classes.iloc[a].values[0] or \"fr_FR\" in faker_classes.iloc[a].values[0] or \"de_DE\" in faker_classes.iloc[a].values[0]:\n",
    "        val = faker_classes.iloc[a].values[0][:-6]\n",
    "    else:\n",
    "        val = faker_classes.iloc[a].values[0][:-3]\n",
    "    if val not in faker_check_unique.keys():\n",
    "        keep.append(a)\n",
    "all_dataset = pd.concat([all_dataset, pd.DataFrame(faker_data.iloc[:, keep].shape[1] * [\"faker\"]).rename(columns={0: \"dataset\"})]).reset_index(drop=True)\n",
    "all_data = pd.concat([all_data, faker_data.iloc[:, keep]], axis=1)\n",
    "all_labels = pd.concat([all_labels, faker_labels.iloc[keep]]).reset_index(drop=True)\n",
    "all_classes = pd.concat([all_classes, faker_classes.iloc[keep]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in mimesis_check_unique.keys():\n",
    "    check_set = set(dessi_all.iloc[:,[a for a in dessi_all_classes[\"class\"].index if mimesis_check_unique[val][0] \n",
    "                      in dessi_all_classes[\"class\"].loc[a]]].values.flatten())\n",
    "    for lan in [\"en\", \"de\", \"fr\", \"mixed\"]:\n",
    "        cols = []\n",
    "        for a in range(mimesis_data.shape[1]):\n",
    "            if f\"{val}_{lan}\" == mimesis_classes.iloc[a].values[0]:\n",
    "                cols.append(a)\n",
    "        value_set = set(mimesis_data.iloc[:100, cols].values.flatten())\n",
    "        add_values = value_set - check_set\n",
    "        try:\n",
    "            add_values = sorted(add_values)\n",
    "        except TypeError as e:\n",
    "            add_values = sorted([str(a) for a in add_values])\n",
    "        random.shuffle(add_values)\n",
    "        add_values = add_values[:int(np.floor(len(add_values)/100))*100]\n",
    "        add_df = pd.DataFrame(np.array(add_values).reshape(100, int(len(add_values)/100)))\n",
    "        add_df.columns = mimesis_data.columns[cols[:add_df.shape[1]]]\n",
    "        add_df = add_split_type_equal(add_df)\n",
    "        all_data = pd.concat([all_data, add_df], axis=1)\n",
    "        labels_add = pd.DataFrame([mimesis_check_unique[val][1]]*add_df.shape[1])\n",
    "        labels_add.columns = [\"label\"]\n",
    "        labels_add = add_split_type_equal(labels_add)\n",
    "        all_labels = pd.concat([all_labels, labels_add], ignore_index=True)\n",
    "        classes_add = pd.DataFrame([f\"{val}_{lan}\"]*add_df.shape[1])\n",
    "        classes_add.columns = [\"class\"]\n",
    "        classes_add = add_split_type_equal(classes_add)\n",
    "        all_classes = pd.concat([all_classes, classes_add], ignore_index=True)\n",
    "        all_dataset = pd.concat([all_dataset, pd.DataFrame([\"mimesis\"]*add_df.shape[1]).rename(columns={0: \"dataset\"})], ignore_index=True)\n",
    "        \n",
    "for val in faker_check_unique.keys():\n",
    "    if faker_check_unique[val][0] != None:\n",
    "        check_set = set(dessi_all.iloc[:,[a for a in dessi_all_classes[\"class\"].index if faker_check_unique[val][0] \n",
    "                      in dessi_all_classes[\"class\"].loc[a]]].values.flatten())\n",
    "    else:\n",
    "        check_set = set()\n",
    "    if faker_check_unique[val][1] != None:\n",
    "        s1 = set([a for a in mimesis_all_classes[\"class\"] if faker_check_unique[val][1] in a])\n",
    "        check_set2 = set(mimesis_all.iloc[:, mimesis_all_classes.index[mimesis_all_classes[\"class\"].isin(s1)]].values.flatten())\n",
    "        check_set = check_set.union(check_set2)\n",
    "    for lan in [\"en\", \"de_DE\", \"fr_FR\", \"mixed\"]:\n",
    "        cols = []\n",
    "        for a in range(faker_data.shape[1]):\n",
    "            if f\"{val}_{lan}\" == faker_classes.iloc[a].values[0]:\n",
    "                cols.append(a)\n",
    "        value_set = set(faker_data.iloc[:100, cols].values.flatten())\n",
    "        add_values = value_set - check_set\n",
    "        try:\n",
    "            add_values = sorted(add_values)\n",
    "        except TypeError as e:\n",
    "            add_values = sorted([str(a) for a in add_values])\n",
    "        random.shuffle(add_values)\n",
    "        add_values = add_values[:int(np.floor(len(add_values)/100))*100]\n",
    "        add_df = pd.DataFrame(np.array(add_values).reshape(100, int(len(add_values)/100)))\n",
    "        add_df.columns = faker_data.columns[cols[:add_df.shape[1]]]\n",
    "        add_df = add_split_type_equal(add_df)\n",
    "        all_data = pd.concat([all_data, add_df], axis=1)\n",
    "        labels_add = pd.DataFrame([faker_check_unique[val][2]]*add_df.shape[1])\n",
    "        labels_add.columns = [\"label\"]\n",
    "        labels_add = add_split_type_equal(labels_add)\n",
    "        all_labels = pd.concat([all_labels, labels_add], ignore_index=True)\n",
    "        classes_add = pd.DataFrame([f\"{val}_{lan}\"]*add_df.shape[1])\n",
    "        classes_add.columns = [\"class\"]\n",
    "        classes_add = add_split_type_equal(classes_add)\n",
    "        all_classes = pd.concat([all_classes, classes_add], ignore_index=True)\n",
    "        all_dataset = pd.concat([all_dataset, pd.DataFrame([\"faker\"]*add_df.shape[1]).rename(columns={0: \"dataset\"})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shuffled_splitted_data(data, labels, classes, dataset, split_type):\n",
    "    data = data.iloc[:, [a for a in range(data.shape[1]) if data.iloc[100,a] == split_type]]\n",
    "    labels = labels.loc[labels[\"split_type\"] == split_type]\n",
    "    classes = classes.loc[classes[\"split_type\"] == split_type]\n",
    "    dataset = dataset.loc[classes.loc[classes[\"split_type\"] == split_type].index]\n",
    "    data = data.sample(frac=1, axis=1, random_state=42).reset_index(drop=True)\n",
    "    labels = labels.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    classes = classes.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return data.iloc[:100,:], labels.iloc[:,:1], classes.iloc[:,:1], dataset\n",
    "\n",
    "train_all, train_all_labels, train_all_classes, train_all_dataset = get_shuffled_splitted_data(all_data, all_labels, all_classes, all_dataset, \"train\")\n",
    "train_all.to_csv(\"train.csv\", index=False)\n",
    "train_all_labels.to_csv(\"train_labels_personal.csv\", index=False)\n",
    "train_all_classes.to_csv(\"train_classes.csv\", index=False)\n",
    "train_all_dataset.to_csv(\"train_dataset.csv\", index=False)\n",
    "\n",
    "dev_all, dev_all_labels, dev_all_classes, dev_all_dataset = get_shuffled_splitted_data(all_data, all_labels, all_classes, all_dataset, \"dev\")\n",
    "dev_all.to_csv(\"dev.csv\", index=False)\n",
    "dev_all_labels.to_csv(\"dev_labels_personal.csv\", index=False)\n",
    "dev_all_classes.to_csv(\"dev_classes.csv\", index=False)\n",
    "dev_all_dataset.to_csv(\"dev_dataset.csv\", index=False)\n",
    "\n",
    "test_all, test_all_labels, test_all_classes, test_all_dataset = get_shuffled_splitted_data(all_data, all_labels, all_classes, all_dataset, \"test\")\n",
    "test_all.to_csv(\"test.csv\", index=False)\n",
    "test_all_labels.to_csv(\"test_labels_personal.csv\", index=False)\n",
    "test_all_classes.to_csv(\"test_classes.csv\", index=False)\n",
    "test_all_dataset.to_csv(\"test_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Multiclass Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_multiclass = {\n",
    "    \"ccn\": \"credit_card_number\",\n",
    "    \"current_location\": \"longitude_and_latitude\",\n",
    "    \"geolocation\": \"longitude_and_latitude\",\n",
    "    \"name\": \"full_name\",\n",
    "    \"isbn13\": \"isbn\",\n",
    "    \"nin\": \"national_identification_number\",   #rename some classes so that GPT can understand them\n",
    "    \"ssn\": \"national_identification_number\",\n",
    "    \"pyfloat\": \"float_number\",\n",
    "    \"pyint\": \"integer_number\",\n",
    "    \"swift\": \"SWIFT/BIC code\",\n",
    "    \"swift/bic\": \"SWIFT/BIC code\",\n",
    "    \"address\": \"full_address\",\n",
    "    \"ean\": \"EAN_code\",\n",
    "    \"occupation\": \"job\",\n",
    "    \"organization\": \"company\",\n",
    "    \"organization,phone_number\": \"company,phone_number\",\n",
    "    \"passport\": \"passport_number\",\n",
    "    \"religion\": \"religion/worldview\",   #values in these columns contain both classes\n",
    "    \"worldview\": \"religion/worldview\",\n",
    "    \"academic_degree\": \"academic_degree/title\",          \n",
    "    \"title\": \"academic_degree/title\",\n",
    "    \"blood_type\": \"blood_group\",\n",
    "    \"sex\": \"gender\"\n",
    "}\n",
    "\n",
    "def convert_classes(data_classes):\n",
    "    new_classes = []\n",
    "    for i in data_classes[\"class\"]:\n",
    "        if \"mixed\" in i or \"de_DE\" in i or \"fr_FR\" in i:\n",
    "            new_classes.append(i[:-6].lower())\n",
    "        elif \"_en\" in i or \"_de\" in i or \"_fr\" in i:\n",
    "            new_classes.append(i[:-3].lower())\n",
    "        else:\n",
    "            new_classes.append(i.lower())\n",
    "    for i in range(len(new_classes)):\n",
    "        for a in mapping_multiclass.keys():\n",
    "            if (a == new_classes[i]) or (a in new_classes[i] and \",\" in new_classes[i]):\n",
    "                new_classes[i] = new_classes[i].replace(a, mapping_multiclass[a])\n",
    "    return pd.DataFrame(new_classes).rename(columns={0: \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "convert_classes(train_all_classes).to_csv(\"train_labels_multi.csv\", index=False)\n",
    "convert_classes(dev_all_classes).to_csv(\"dev_labels_multi.csv\", index=False)\n",
    "convert_classes(test_all_classes).to_csv(\"test_labels_multi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
