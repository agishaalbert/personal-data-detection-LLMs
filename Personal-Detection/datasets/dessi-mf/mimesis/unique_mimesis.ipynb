{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection  import StratifiedShuffleSplit\n",
    "import string\n",
    "from mimesis.locales import Locale\n",
    "from mimesis import Fieldset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mimesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Locale.AR_AE: 'ar-ae'>, <Locale.AR_DZ: 'ar-dz'>, <Locale.AR_EG: 'ar-eg'>, <Locale.AR_JO: 'ar-jo'>, <Locale.AR_OM: 'ar-om'>, <Locale.AR_SY: 'ar-sy'>, <Locale.AR_YE: 'ar-ye'>, <Locale.CS: 'cs'>, <Locale.DA: 'da'>, <Locale.DE: 'de'>, <Locale.DE_AT: 'de-at'>, <Locale.DE_CH: 'de-ch'>, <Locale.EL: 'el'>, <Locale.EN: 'en'>, <Locale.EN_AU: 'en-au'>, <Locale.EN_CA: 'en-ca'>, <Locale.EN_GB: 'en-gb'>, <Locale.ES: 'es'>, <Locale.ES_MX: 'es-mx'>, <Locale.ET: 'et'>, <Locale.FA: 'fa'>, <Locale.FI: 'fi'>, <Locale.FR: 'fr'>, <Locale.HU: 'hu'>, <Locale.HR: 'hr'>, <Locale.IS: 'is'>, <Locale.IT: 'it'>, <Locale.JA: 'ja'>, <Locale.KK: 'kk'>, <Locale.KO: 'ko'>, <Locale.NL: 'nl'>, <Locale.NL_BE: 'nl-be'>, <Locale.NO: 'no'>, <Locale.PL: 'pl'>, <Locale.PT: 'pt'>, <Locale.PT_BR: 'pt-br'>, <Locale.RU: 'ru'>, <Locale.SK: 'sk'>, <Locale.SV: 'sv'>, <Locale.TR: 'tr'>, <Locale.UK: 'uk'>, <Locale.ZH: 'zh'>]\n"
     ]
    }
   ],
   "source": [
    "print(list(Locale)) #print languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use French, German and English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I picked some classes which clearly fits to personal or non-personal data without the information of the column name (most classes are from mimesis.Person). Half of the column names are imputed with random strings to ensure the column name is not decisive for the models prediction.\n",
    "  \n",
    "--> 16 classes for personal attributed, 21 for non-personal\n",
    "\n",
    "For some classes the restriction is added that all values in the whole datasets must be unique. This should ensure that the predictions of the BERT model are not dependent on specific words for these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_attributes = [\n",
    "    \"address\", \"academic_degree\", \"blood_type\", \"email\", \"first_name\", \"full_name\", \"last_name\", \n",
    "    \"gender\", \"language\", \"nationality\", \"occupation\",\n",
    "    \"phone_number\", \"political_views\", \"title\", \"worldview\", \n",
    "    \"credit_card_number\"\n",
    "]\n",
    "\n",
    "non_personal_attributes = [\n",
    "    \"company\", \"dish\", \"drink\", \"answer\", \"color\", \"isbn\", \"duration\", \n",
    "    \"programming_language\", \"system_quality_attribute\", \"version\", \"float_number\", \n",
    "    \"integer_number\", \"user_agent\", \"graphics\", \"cpu\", \"phone_model\", \n",
    "    \"manufacturer\", \"resolution\", \"word\", \"measure_unit\", \n",
    "    \"city\"\n",
    "]\n",
    "\n",
    "personal_attributes_unique = [\n",
    "            \"address\", \"email\", \"full_name\",\n",
    "            \"phone_number\", \"credit_card_number\"\n",
    "        ]\n",
    "\n",
    "non_personal_attributes_unique = [\"isbn\", \"version\", \"float_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "address 99703\n",
      "email 99798\n",
      "full_name 83704\n",
      "phone_number 100000\n",
      "credit_card_number 100000\n",
      "isbn 99999\n",
      "version 95172\n",
      "float_number 100000\n"
     ]
    }
   ],
   "source": [
    "fieldset = Fieldset(locale=\"de\", seed=42)\n",
    "for p in personal_attributes + non_personal_attributes:\n",
    "     vals = fieldset(p, i = 100000)\n",
    "     if len(set(vals)) > 50000:\n",
    "         print(p, len(set(vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate fieldsets as constants to avoid that data are generated equally\n",
    "FIELDSET_EN = Fieldset(locale = \"en\", seed = 42)\n",
    "FIELDSET_FR = Fieldset(locale = \"fr\", seed = 42)\n",
    "FIELDSET_DE = Fieldset(locale = \"de\", seed = 42)\n",
    "random.seed(42)\n",
    "    \n",
    "\n",
    "def generate_data(classes, num_col, personal_type, df, original_class, personal):\n",
    "    for cla in classes:\n",
    "        f1 = Fieldset(locale=\"en\", seed=42)\n",
    "        f2 = Fieldset(locale=\"fr\", seed=42)\n",
    "        if f1(cla, i=1) == f2(cla, i=1):\n",
    "            fieldset = FIELDSET_EN\n",
    "            for lan in  [\"en\",\"fr\",\"de\", \"mixed\"]:\n",
    "                for i in range(1,num_col):                           \n",
    "                    data = {\n",
    "                            f\"{cla}_{lan}_{i}\": [],\n",
    "                        }\n",
    "                    data[f\"{cla}_{lan}_{i}\"] = fieldset(cla, i = 100)\n",
    "                    personal_type.append(personal)\n",
    "                    original_class.append(f\"{cla}_{lan}\")\n",
    "                    df = pd.concat([df, pd.DataFrame(data)], axis=1)\n",
    "        else:\n",
    "            for (fieldset, lan) in  zip([FIELDSET_EN, FIELDSET_FR, FIELDSET_DE, FIELDSET_EN], [\"en\",\"fr\",\"de\", \"mixed\"]):\n",
    "                for i in range(1,num_col):                           \n",
    "                    data = {\n",
    "                            f\"{cla}_{lan}_{i}\": [],\n",
    "                        }\n",
    "                    if lan != \"mixed\":\n",
    "                        data[f\"{cla}_{lan}_{i}\"] = fieldset(cla, i = 100)\n",
    "                    else:\n",
    "                        for (fieldset, lan2) in zip([FIELDSET_EN, FIELDSET_FR, FIELDSET_DE], [\"en\",\"fr\",\"de\"]):\n",
    "                            values = fieldset(cla, i = 33 if lan2 != \"en\" else 34)\n",
    "                            for v in values:\n",
    "                                data[f\"{cla}_mixed_{i}\"].append(v)\n",
    "                    personal_type.append(personal)\n",
    "                    original_class.append(f\"{cla}_{lan}\")\n",
    "                    df = pd.concat([df, pd.DataFrame(data)], axis=1)\n",
    "    return personal_type, df, original_class\n",
    "\n",
    "\n",
    "def generate_data_unique(classes, num_col, personal_type, df, original_class, personal):\n",
    "    mixed_data = dict()\n",
    "    for cla in classes:    \n",
    "        f1 = Fieldset(locale=\"en\", seed=42)\n",
    "        f2 = Fieldset(locale=\"fr\", seed=42)\n",
    "        if f1(cla, i=1) == f2(cla, i=1):\n",
    "            fieldset = FIELDSET_EN\n",
    "            values = set()\n",
    "            num_generate = (num_col *4.5)\n",
    "            while len(values) < num_generate*100:\n",
    "                values.update(fieldset(cla, i = num_col*100))\n",
    "            values = sorted(values)\n",
    "            random.shuffle(values)\n",
    "            for lan in  [\"en\",\"fr\",\"de\", \"mixed\"]:\n",
    "                for i in range(1,num_col):                           \n",
    "                    data = {\n",
    "                            f\"{cla}_{lan}_{i}\": [],\n",
    "                        }\n",
    "                    popped_elements = values[:100]\n",
    "                    values = values[100:]\n",
    "                    data[f\"{cla}_{lan}_{i}\"] = popped_elements\n",
    "                    personal_type.append(personal)\n",
    "                    original_class.append(f\"{cla}_{lan}\")\n",
    "                    df = pd.concat([df, pd.DataFrame(data)], axis=1)\n",
    "        else:\n",
    "            all_values = set()\n",
    "            for (fieldset, lan) in zip([FIELDSET_EN, FIELDSET_FR, FIELDSET_DE], [\"en\",\"fr\",\"de\"]):\n",
    "                values = set()\n",
    "                num_generate = (num_col *1.5)\n",
    "                while len(values) < num_generate*100:\n",
    "                    new_values = set(fieldset(cla, i = num_col*100)) - all_values\n",
    "                    values.update(new_values)\n",
    "                values = sorted(values)\n",
    "                random.shuffle(values)\n",
    "                all_values = all_values.union(set(values))\n",
    "                for i in range(1,num_col):                           \n",
    "                    data = {\n",
    "                            f\"{cla}_{lan}_{i}\": [],\n",
    "                        }\n",
    "                    popped_elements = values[:100]\n",
    "                    values = values[100:]\n",
    "                    data[f\"{cla}_{lan}_{i}\"] = popped_elements\n",
    "                    personal_type.append(personal)\n",
    "                    original_class.append(f\"{cla}_{lan}\")\n",
    "                    \n",
    "                    # Concatenate the new data to the main DataFrame\n",
    "                    df = pd.concat([df, pd.DataFrame(data)], axis=1)\n",
    "                mixed_data[f'{lan}_{cla}'] = values\n",
    "            for i in range(1,num_col):                           \n",
    "                data = {\n",
    "                        f\"{cla}_mixed_{i}\": [],\n",
    "                    }\n",
    "                for lan in [\"en\",\"fr\",\"de\"]:\n",
    "                    if lan == \"en\":\n",
    "                        num=34\n",
    "                    else:\n",
    "                        num=33\n",
    "                    for v in mixed_data[f'{lan}_{cla}'][num*(i-1):num*(i-1)+num]:\n",
    "                        data[f\"{cla}_mixed_{i}\"].append(v)\n",
    "                personal_type.append(personal)\n",
    "                original_class.append(f\"{cla}_mixed\")\n",
    "                \n",
    "                # Concatenate the new data to the main DataFrame\n",
    "                df = pd.concat([df, pd.DataFrame(data)], axis=1)\n",
    "    return personal_type, df, original_class\n",
    "    \n",
    "    \n",
    "def generate_random_string(length):\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(characters) for _ in range(length))\n",
    "    \n",
    "def rename_columns(df, num_personal, num_non_personal):\n",
    "    for attributes in [personal_attributes, non_personal_attributes]:\n",
    "        for p in attributes:\n",
    "            if attributes == personal_attributes:\n",
    "                num_col = num_personal\n",
    "            else:\n",
    "                num_col = num_non_personal\n",
    "            for lan in [\"en\",\"fr\",\"de\", \"mixed\"]:\n",
    "                for i in range(1,num_col, 2):\n",
    "                    random_length = random.randint(5, 20)\n",
    "                    random_string = generate_random_string(random_length)\n",
    "                    df.rename(columns={f\"{p}_{lan}_{i}\": random_string}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def extract_label(df):\n",
    "    df_labels = df.iloc[100,:]\n",
    "    df_classes = df.iloc[101,:]\n",
    "    df = df.iloc[:100,:]\n",
    "    df_labels = pd.DataFrame(df_labels)\n",
    "    df_classes = pd.DataFrame(df_classes)\n",
    "    df_labels = df_labels.rename(columns={df_labels.columns[0]: \"label\"}).reset_index(drop=True)\n",
    "    df_classes = df_classes.rename(columns={df_classes.columns[0]: \"class\"}).reset_index(drop=True)\n",
    "    return df, df_labels, df_classes\n",
    "\n",
    "def create_dataset():\n",
    "    personal_type = []\n",
    "    original_class = []\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # generate columns were only unique values are impossible\n",
    "    personal_type, df, original_class = generate_data(set(personal_attributes) - set(personal_attributes_unique), \n",
    "                                                      250, personal_type, df, original_class, \"personal\")\n",
    "    personal_type, df, original_class = generate_data(set(non_personal_attributes) - set(non_personal_attributes_unique), \n",
    "                                                      191, personal_type, df, original_class, \"non-personal\")\n",
    "    \n",
    "    # generate columns were only unique values are possible\n",
    "    personal_type, df, original_class = generate_data_unique(personal_attributes_unique,\n",
    "                                                      250, personal_type, df, original_class, \"personal\")\n",
    "    personal_type, df, original_class = generate_data_unique(non_personal_attributes_unique,\n",
    "                                                      191, personal_type, df, original_class, \"non-personal\")\n",
    "    \n",
    "   \n",
    "    df = rename_columns(df, 250, 191)\n",
    "\n",
    "    #shuffle the columns\n",
    "    labels = pd.DataFrame(personal_type).T\n",
    "    classes = pd.DataFrame(original_class).T\n",
    "    labels.columns = df.columns\n",
    "    classes.columns = df.columns\n",
    "    df = pd.concat([df, labels]).reset_index(drop=True)\n",
    "    df = pd.concat([df, classes]).reset_index(drop=True)\n",
    "    df = df.sample(frac=1, axis=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    #save the data\n",
    "    df_final, labels, classes = extract_label(df)\n",
    "    df_final.to_csv(\"all.csv\", index=False)\n",
    "    labels.to_csv(\"all_labels.csv\", index=False)\n",
    "    classes.to_csv(\"all_classes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Corrupt 50% of the column names for every class   \n",
    "- Train-Val-Test-split (60/20/20)  -> like dessi\n",
    "- try generating like dessi 18k/6k/6k columns\n",
    "- Shuffle the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some classes like floats_integers are language independent.  \n",
    "They are created the same way with every language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250.0\n",
      "190.47619047619048\n"
     ]
    }
   ],
   "source": [
    "#Generate for on personal attribute 250 columns and for a non-personal attribute 191\n",
    "print(16000 / len(personal_attributes) /4)\n",
    "print(16000 / len(non_personal_attributes) /4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\AppData\\Local\\Temp\\ipykernel_24832\\4232468530.py:1: DtypeWarning: Columns (6346,6608,7526,8223,12006,13916,14070,14309,14599,15658,17137,19020,19028,19184,19398,19660,19802,21886,23500,25335,25399,26061,27230,27308,27995,28578,29593,30131,30285,31249) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_all = pd.read_csv(\"mimesis/all.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv(\"all.csv\")\n",
    "df_all_labels = pd.read_csv(\"all_labels.csv\")\n",
    "df_all_classes = pd.read_csv(\"all_classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (100, 19137)\n",
      "Validation data shape: (100, 6379)\n",
      "Test data shape: (100, 6380)\n"
     ]
    }
   ],
   "source": [
    "split1 = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=42)\n",
    "train_idx, temp_idx = next(split1.split(df_all.T, df_all_classes[\"class\"]))\n",
    "train_data = df_all.iloc[:, train_idx]\n",
    "train_classes = df_all_classes.T[train_idx].T\n",
    "train_labels = df_all_labels.T[train_idx].T\n",
    "temp_data = df_all.iloc[:, temp_idx]  \n",
    "temp_classes = df_all_classes.T[temp_idx].T\n",
    "temp_labels = df_all_labels.T[temp_idx].T\n",
    "\n",
    "split2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_idx, test_idx = next(split2.split(temp_data.T, temp_classes))\n",
    "\n",
    "val_data = temp_data.iloc[:, val_idx]\n",
    "val_classes = temp_classes.T.iloc[:,val_idx].T\n",
    "val_labels = temp_labels.T.iloc[:,val_idx].T\n",
    "test_data = temp_data.iloc[:, test_idx]\n",
    "test_classes = temp_classes.T.iloc[:,test_idx].T\n",
    "test_labels = temp_labels.T.iloc[:,test_idx].T\n",
    "\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train.csv\", index=False)\n",
    "train_labels.to_csv(\"train_labels.csv\", index=False)\n",
    "train_classes.to_csv(\"train_classes.csv\", index=False)\n",
    "\n",
    "val_data.to_csv(\"dev.csv\", index=False)\n",
    "val_labels.to_csv(\"dev_labels.csv\", index=False)\n",
    "val_classes.to_csv(\"dev_classes.csv\", index=False)\n",
    "\n",
    "test_data.to_csv(\"test.csv\", index=False)\n",
    "test_labels.to_csv(\"test_labels.csv\", index=False)\n",
    "test_classes.to_csv(\"test_classes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
