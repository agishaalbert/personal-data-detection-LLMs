{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect ten random datasets of OpenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import openml\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ten most suitable datasets out of the twenty:  \n",
    "- 41710, 41767, 41818 are all FOREX datasets, one is enough   \n",
    "- 44326 is an image dataset  \n",
    "- 802, 44682, 45652, 260, 754 is lacking a description to classify the columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41729, 44962, 43557, 42055, 854, 1016, 44058, 46075, 222, 287]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list = openml.datasets.list_datasets(output_format='dataframe')\n",
    "\n",
    "# Select 10 random datasets\n",
    "random.seed(42)                                    # limit NumberOfFeatures to 25 to avoid intensive manual labeling\n",
    "dataset_filtered = dataset_list.loc[(dataset_list[\"NumberOfFeatures\"] <= 25)]\n",
    "random_dataset_ids = random.sample(dataset_filtered['did'].tolist(), 20) \n",
    "random_dataset_ids = list(set(random_dataset_ids) - {41710, 41767, 41818, 44326, 802, 44682, 45652, 719, 260, 754})\n",
    "random_dataset_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = random_dataset_ids               \n",
    "for id in ids:\n",
    "    dataset = openml.datasets.get_dataset(id)\n",
    "    name = str(dataset.name)\n",
    "    os.makedirs(f\"{name}\", exist_ok=True)\n",
    "    X, y, _, _ = dataset.get_data()\n",
    "    pd.concat([X, y], axis=1).to_csv(f\"{name}/data.csv\", index=False)\n",
    "    metadata_dict = {\"Dataset Name: \" : name,\n",
    "                     \"Description: \": str(dataset.description),\n",
    "                     \"Features: \": str(list(dataset.features.values()))\n",
    "                     }\n",
    "    with open(f\"{name}/metadata.json\", 'w') as file:\n",
    "        json.dump(metadata_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automatically label the column as personal-related if it is from the first ten datasets       \n",
    "Personal-related is in this context data that can be used in combination with other data to identify a person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_labels_personal():\n",
    "    folders = [name for name in os.listdir(\".\") if os.path.isdir(os.path.join(\".\", name))]\n",
    "    for folder in folders:\n",
    "        path = folder\n",
    "        csv_file = \"/data.csv\"\n",
    "        df = pd.read_csv(path + \"/\" + csv_file)\n",
    "        columns_personal = dict()\n",
    "        #first label everything as non-personal automatically and then relabel them afterwards manually in the json file\n",
    "        lab = \"non-personal\"            \n",
    "        for c in df.columns:\n",
    "            columns_personal[c] = lab\n",
    "        columns_personal[\"overall\"] = lab\n",
    "        with open(f'{folder}/{csv_file}-labels_personal.json', 'w') as file:\n",
    "            json.dump(columns_personal, file, indent=4)\n",
    "            \n",
    "#line to execute function is commented so that it is not always executed, labeling process only happens once\n",
    "#create_json_labels_personal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOME LABELS WERE ADJUSTED MANUALLY AFTERWARDS IN THE JSON FILES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the labels into a suitable csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [name for name in os.listdir(\".\") if os.path.isdir(os.path.join(\".\", name))]\n",
    "for folder in folders:\n",
    "    path_folder = folder\n",
    "    csv_file = [f for f in os.listdir(path_folder) if f.endswith('.csv') and 'labels' not in f][0]\n",
    "    path = path_folder + \"/\" + csv_file + \"-labels_personal.json\"\n",
    "    with open(path, 'r') as file:\n",
    "        labels_json = json.load(file)\n",
    "    pd.DataFrame(labels_json, index=[0]).T.rename(columns={0: \"label\"}).to_csv(path_folder + \"/labels_personal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all datasets and labels for CASSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [name for name in os.listdir(\".\") if os.path.isdir(os.path.join(\".\", name))]\n",
    "all_dfs = pd.DataFrame()\n",
    "all_dfs_labels = pd.DataFrame()\n",
    "dataset_name = []\n",
    "for folder in folders:\n",
    "    path_folder = folder\n",
    "    csv_file = [f for f in os.listdir(path_folder) if f.endswith('.csv') and 'labels' not in f][0]\n",
    "    csv_file_label = [f for f in os.listdir(path_folder) if f.endswith('.csv') and 'labels_personal' in f][0]\n",
    "    df_add = pd.read_csv(path_folder + \"/\" + csv_file)\n",
    "    all_dfs = pd.concat([all_dfs, df_add.iloc[:100,:]], axis = 1)\n",
    "    add_dfs_labels = pd.read_csv(path_folder + \"/\" + csv_file_label)\n",
    "    all_dfs_labels = pd.concat([all_dfs_labels, add_dfs_labels.iloc[:-1]]).reset_index(drop=True)\n",
    "    dataset_name += [folder] * df_add.shape[1]\n",
    "all_dfs.to_csv(\"all_datasets.csv\", index=False)\n",
    "all_dfs_labels.to_csv(\"all_datasets_labels_personal.csv\", index=False)\n",
    "pd.DataFrame(dataset_name).rename(columns={0: \"dataset\"}).to_csv(\"all_datasets_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automatically label the column as pii if it only contains unique values  \n",
    "Afterwards look trough all .json files and check the column labeling      \n",
    "only columns which can contain personal identifiable information without a combination with other information are marked as pii "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_json_labels_pii():\n",
    "    folders = [name for name in os.listdir(\".\") if os.path.isdir(os.path.join(\".\", name))]\n",
    "    for folder in folders:\n",
    "        path = folder\n",
    "        csv_file = [f for f in os.listdir(path) if f.endswith('.csv') and 'labels' not in f][0]\n",
    "        with open(path + \"/\" + csv_file, 'r') as file:\n",
    "            first_line = file.readline()\n",
    "            comma_count = first_line.count(',')\n",
    "            semicolon_count = first_line.count(';')\n",
    "            if comma_count > semicolon_count:\n",
    "                sep = \",\"\n",
    "            else:\n",
    "                sep = \";\"\n",
    "        df = pd.read_csv(path + \"/\" + csv_file, sep=sep)\n",
    "        columns_personal = dict()\n",
    "        for c in df.columns:\n",
    "            columns_personal[c] = \"non-pii\"\n",
    "        columns_personal[\"overall\"] = \"pii\"\n",
    "        with open(f'{folder}/{csv_file}-labels_pii.json', 'w') as file:\n",
    "            json.dump(columns_personal, file, indent=4)\n",
    "#line to execute function is commented so that it is not always executed, labeling process only happens once\n",
    "#create_json_labels_pii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOME LABELS WERE ADJUSTED MANUALLY AFTERWARDS IN THE JSON FILES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the labels into a suitable csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [name for name in os.listdir(\".\") if os.path.isdir(os.path.join(\".\", name))]\n",
    "for folder in folders:\n",
    "    path_folder = folder\n",
    "    csv_file = [f for f in os.listdir(path_folder) if f.endswith('.csv') and 'labels' not in f][0]\n",
    "    path = path_folder + \"/\" + csv_file + \"-labels_pii.json\"\n",
    "    with open(path, 'r') as file:\n",
    "        labels_json = json.load(file)\n",
    "    pd.DataFrame(labels_json, index=[0]).T.rename(columns={0: \"label\"}).to_csv(path_folder + \"/labels_pii.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all datasets and labels for CASSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [name for name in os.listdir(\".\") if os.path.isdir(os.path.join(\".\", name))]\n",
    "all_dfs = pd.DataFrame()\n",
    "all_dfs_labels = pd.DataFrame()\n",
    "for folder in folders:\n",
    "    path_folder = folder\n",
    "    csv_file = [f for f in os.listdir(path_folder) if f.endswith('.csv') and 'labels' not in f][0]\n",
    "    csv_file_label = [f for f in os.listdir(path_folder) if f.endswith('.csv') and 'labels_pii' in f][0]\n",
    "    \n",
    "    with open(path_folder + \"/\" + csv_file, 'r') as file:\n",
    "        first_line = file.readline()\n",
    "        comma_count = first_line.count(',')\n",
    "        semicolon_count = first_line.count(';')\n",
    "        if comma_count > semicolon_count:\n",
    "            sep = \",\"\n",
    "        else:\n",
    "            sep = \";\"\n",
    "    df_add = pd.read_csv(path_folder + \"/\" + csv_file, sep=sep)\n",
    "    \n",
    "    all_dfs = pd.concat([all_dfs, df_add.iloc[:100,:]], axis = 1)\n",
    "    add_dfs_labels = pd.read_csv(path_folder + \"/\" + csv_file_label)\n",
    "    all_dfs_labels = pd.concat([all_dfs_labels, add_dfs_labels.iloc[:-1]]).reset_index(drop=True)\n",
    "all_dfs.to_csv(\"all_datasets.csv\", index=False)\n",
    "all_dfs_labels.to_csv(\"all_datasets_labels_pii.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
