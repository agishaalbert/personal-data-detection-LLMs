{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        The File gpt.py provides the necessary functions for the batching and the run of the gpt LLM on the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"OpenAIAPIKey.txt\", \"r\") as f:\n",
    "#     openai.api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm = pd.read_csv('../datasets/dessi-mf/dessi-mf_gpt/test.csv')\n",
    "labels_llm = pd.read_csv('../datasets/dessi-mf/dessi-mf_gpt/test_labels_personal.csv')\n",
    "classes_llm = pd.read_csv('../datasets/dessi-mf/dessi-mf_gpt/test_classes.csv')\n",
    "multiclass_llm = pd.read_csv('../datasets/dessi-mf/dessi-mf_gpt/test_labels_multiclass.csv')\n",
    "\n",
    "test_languages_data = pd.read_csv('../datasets/test_languages/test.csv')\n",
    "test_languages_labels = pd.read_csv('../datasets/test_languages/test_labels_personal.csv')\n",
    "test_languages_classes = pd.read_csv('../datasets/test_languages/test_classes.csv')\n",
    "test_languages_multiclass = pd.read_csv('../datasets/test_languages/test_labels_multiclass.csv')\n",
    "\n",
    "kaggle_data = pd.read_csv('../datasets/kaggle_datasets/all_datasets.csv')\n",
    "kaggle_labels = pd.read_csv('../datasets/kaggle_datasets/all_datasets_labels_personal.csv')\n",
    "kaggle_dataset = pd.read_csv('../datasets/kaggle_datasets/all_datasets_names.csv')\n",
    "\n",
    "openml_data = pd.read_csv('../datasets/openml_datasets/all_datasets.csv')\n",
    "openml_labels = pd.read_csv('../datasets/openml_datasets/all_datasets_labels_personal.csv')\n",
    "openml_dataset = pd.read_csv('../datasets/openml_datasets/all_datasets_names.csv')\n",
    "\n",
    "openml_2_data = pd.read_csv('../datasets/openml_datasets_2/all_datasets.csv')\n",
    "openml_2_labels = pd.read_csv('../datasets/openml_datasets_2/all_datasets_labels_personal.csv')\n",
    "openml_2_dataset = pd.read_csv('../datasets/openml_datasets_2/all_datasets_names.csv')\n",
    "\n",
    "medical_data = pd.read_csv('../datasets/freiburg-medical/test.csv')\n",
    "medical_labels = pd.read_csv('../datasets/freiburg-medical/test_labels_personal.csv')\n",
    "\n",
    "openmlall_data = pd.read_csv('../datasets/OpenMLall_dataset/OpenMLAll_data.csv')\n",
    "openmlall_labels = pd.read_csv('../datasets/OpenMLall_dataset/OpenMLAll_data_labels.csv')\n",
    "openmlall_datasets = pd.read_csv('../datasets/OpenMLall_dataset/openmlall_datasets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Email Address</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date of Birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Location</th>\n",
       "      <th>Membership Start Date</th>\n",
       "      <th>Membership End Date</th>\n",
       "      <th>Subscription Plan</th>\n",
       "      <th>...</th>\n",
       "      <th>citric.acid</th>\n",
       "      <th>residual.sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free.sulfur.dioxide</th>\n",
       "      <th>total.sulfur.dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ronald Murphy</td>\n",
       "      <td>williamholland@example.com</td>\n",
       "      <td>williamholland</td>\n",
       "      <td>1953-06-03</td>\n",
       "      <td>Male</td>\n",
       "      <td>Rebeccachester</td>\n",
       "      <td>2024-01-15</td>\n",
       "      <td>2025-01-14</td>\n",
       "      <td>Annual</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Scott Allen</td>\n",
       "      <td>scott22@example.org</td>\n",
       "      <td>scott22</td>\n",
       "      <td>1978-07-08</td>\n",
       "      <td>Male</td>\n",
       "      <td>Mcphersonview</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>2025-01-06</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Jonathan Parrish</td>\n",
       "      <td>brooke16@example.org</td>\n",
       "      <td>brooke16</td>\n",
       "      <td>1994-12-06</td>\n",
       "      <td>Female</td>\n",
       "      <td>Youngfort</td>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>2025-04-13</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Megan Williams</td>\n",
       "      <td>elizabeth31@example.net</td>\n",
       "      <td>elizabeth31</td>\n",
       "      <td>1964-12-22</td>\n",
       "      <td>Female</td>\n",
       "      <td>Feliciashire</td>\n",
       "      <td>2024-01-24</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Kathryn Brown</td>\n",
       "      <td>pattersonalexandra@example.org</td>\n",
       "      <td>pattersonalexandra</td>\n",
       "      <td>1961-06-04</td>\n",
       "      <td>Male</td>\n",
       "      <td>Port Deborah</td>\n",
       "      <td>2024-02-14</td>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>Annual</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Mackenzie Harris</td>\n",
       "      <td>andersonemily@example.net</td>\n",
       "      <td>andersonemily</td>\n",
       "      <td>1963-11-06</td>\n",
       "      <td>Female</td>\n",
       "      <td>Lake Barryland</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Annual</td>\n",
       "      <td>...</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.058</td>\n",
       "      <td>17.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.60</td>\n",
       "      <td>12.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Stephanie Potter</td>\n",
       "      <td>heather10@example.org</td>\n",
       "      <td>heather10</td>\n",
       "      <td>1958-05-05</td>\n",
       "      <td>Female</td>\n",
       "      <td>New Teresaburgh</td>\n",
       "      <td>2024-03-25</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.102</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.56</td>\n",
       "      <td>10.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Mitchell Potter</td>\n",
       "      <td>fsanchez@example.net</td>\n",
       "      <td>fsanchez</td>\n",
       "      <td>1979-08-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>South Katieburgh</td>\n",
       "      <td>2024-01-27</td>\n",
       "      <td>2025-01-26</td>\n",
       "      <td>Annual</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.070</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Jonathan Rodriguez</td>\n",
       "      <td>nichole85@example.net</td>\n",
       "      <td>nichole85</td>\n",
       "      <td>1967-12-20</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Shannonhaven</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.079</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Jennifer Davis</td>\n",
       "      <td>zunigashelly@example.org</td>\n",
       "      <td>zunigashelly</td>\n",
       "      <td>1950-01-15</td>\n",
       "      <td>Female</td>\n",
       "      <td>West Nathanielton</td>\n",
       "      <td>2024-02-14</td>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>Annual</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.080</td>\n",
       "      <td>13.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    User ID                Name                   Email Address  \\\n",
       "0         1       Ronald Murphy      williamholland@example.com   \n",
       "1         2         Scott Allen             scott22@example.org   \n",
       "2         3    Jonathan Parrish            brooke16@example.org   \n",
       "3         4      Megan Williams         elizabeth31@example.net   \n",
       "4         5       Kathryn Brown  pattersonalexandra@example.org   \n",
       "..      ...                 ...                             ...   \n",
       "95       96    Mackenzie Harris       andersonemily@example.net   \n",
       "96       97    Stephanie Potter           heather10@example.org   \n",
       "97       98     Mitchell Potter            fsanchez@example.net   \n",
       "98       99  Jonathan Rodriguez           nichole85@example.net   \n",
       "99      100      Jennifer Davis        zunigashelly@example.org   \n",
       "\n",
       "              Username Date of Birth  Gender           Location  \\\n",
       "0       williamholland    1953-06-03    Male     Rebeccachester   \n",
       "1              scott22    1978-07-08    Male      Mcphersonview   \n",
       "2             brooke16    1994-12-06  Female          Youngfort   \n",
       "3          elizabeth31    1964-12-22  Female       Feliciashire   \n",
       "4   pattersonalexandra    1961-06-04    Male       Port Deborah   \n",
       "..                 ...           ...     ...                ...   \n",
       "95       andersonemily    1963-11-06  Female     Lake Barryland   \n",
       "96           heather10    1958-05-05  Female    New Teresaburgh   \n",
       "97            fsanchez    1979-08-27    Male   South Katieburgh   \n",
       "98           nichole85    1967-12-20    Male  East Shannonhaven   \n",
       "99        zunigashelly    1950-01-15  Female  West Nathanielton   \n",
       "\n",
       "   Membership Start Date Membership End Date Subscription Plan  ...  \\\n",
       "0             2024-01-15          2025-01-14            Annual  ...   \n",
       "1             2024-01-07          2025-01-06           Monthly  ...   \n",
       "2             2024-04-13          2025-04-13           Monthly  ...   \n",
       "3             2024-01-24          2025-01-23           Monthly  ...   \n",
       "4             2024-02-14          2025-02-13            Annual  ...   \n",
       "..                   ...                 ...               ...  ...   \n",
       "95            2024-03-01          2025-03-01            Annual  ...   \n",
       "96            2024-03-25          2025-03-25           Monthly  ...   \n",
       "97            2024-01-27          2025-01-26            Annual  ...   \n",
       "98            2024-02-29          2025-02-28           Monthly  ...   \n",
       "99            2024-02-14          2025-02-13            Annual  ...   \n",
       "\n",
       "   citric.acid residual.sugar chlorides free.sulfur.dioxide  \\\n",
       "0         0.00            1.9     0.076                11.0   \n",
       "1         0.00            2.6     0.098                25.0   \n",
       "2         0.04            2.3     0.092                15.0   \n",
       "3         0.56            1.9     0.075                17.0   \n",
       "4         0.00            1.9     0.076                11.0   \n",
       "..         ...            ...       ...                 ...   \n",
       "95        0.17            2.3     0.058                17.0   \n",
       "96        0.00            3.0     0.102                 8.0   \n",
       "97        0.25            2.0     0.070                 3.0   \n",
       "98        0.06            2.5     0.079                 5.0   \n",
       "99        0.18            1.9     0.080                13.0   \n",
       "\n",
       "   total.sulfur.dioxide density    pH  sulphates  alcohol quality  \n",
       "0                  34.0  0.9978  3.51       0.56      9.4       5  \n",
       "1                  67.0  0.9968  3.20       0.68      9.8       5  \n",
       "2                  54.0  0.9970  3.26       0.65      9.8       5  \n",
       "3                  60.0  0.9980  3.16       0.58      9.8       6  \n",
       "4                  34.0  0.9978  3.51       0.56      9.4       5  \n",
       "..                  ...     ...   ...        ...      ...     ...  \n",
       "95                106.0  0.9932  3.85       0.60     12.9       6  \n",
       "96                 23.0  0.9965  3.45       0.56     10.7       5  \n",
       "97                 22.0  0.9963  3.25       0.63      9.2       5  \n",
       "98                 10.0  0.9967  3.39       0.56      9.8       5  \n",
       "99                 35.0  0.9972  3.30       0.59      9.0       6  \n",
       "\n",
       "[100 rows x 258 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openmlall_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 258)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openmlall_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenML_Concat = pd.concat([openml_data,openml_2_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenMLAll_data_labels = pd.concat([openml_labels,openml_2_labels], axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenMLAll_data_labels.to_csv('OpenMLAll_data_labels.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openmlall_datasets = pd.concat([openml_dataset,openml_2_dataset], axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset                              \n",
       "TVS_Loan_Default                         32\n",
       "APL_20_24                                27\n",
       "Amazon_Prime_Fiction                     19\n",
       "Marvel_Movies_Dataset                    18\n",
       "Avocado-Prices-(Augmented)               14\n",
       "CSM                                      13\n",
       "vowel                                    13\n",
       "forest_fires                             13\n",
       "wine_quality                             12\n",
       "mango_detection_australia                12\n",
       "DATASETBANK                              12\n",
       "FOREX_chfjpy-minute-Close                12\n",
       "nyc-taxi-green-dec-2016                  11\n",
       "company_quality_and_valuation_finance    11\n",
       "echoMonths                               10\n",
       "HousingPrices                             8\n",
       "fishcatch                                 8\n",
       "Oilst_Customers_Dataset                   5\n",
       "iris                                      5\n",
       "FitBit_HeartRate                          3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openmlall_datasets.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenML_Concat.to_csv('OpenMLAll_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sent a request with the CRSRF-method prompt method of Chinese paper of 2023  \n",
    "\n",
    "**Capacity and Role**  \n",
    "As a classifier of personal data in tabular datasets, ...\n",
    "  \n",
    "**Statement**    \n",
    "...  your task is to analyze the provided columns (each containing up to ten distinct values)\n",
    "and determine whether they correspond to a specified semantic data class. \n",
    "If a column matches exactly one of the semantic data classes, return the matching class.\n",
    "Additionally, assess whether the column contains personal information.\n",
    "  \n",
    "**Reason**   \n",
    "Classifying semantic data classes is essential for identifying personal information within tabular data. \n",
    "Detecting personal information helps ensure compliance with data protection regulations and safeguards individuals' privacy and security.\n",
    "   \n",
    "**Format**    \n",
    "Output your results in a dictionary format with two keys:\n",
    "\n",
    "detected_classes: A list of all identified semantic data classes.\n",
    "personal: A boolean indicating whether the column contains data that relates to a natural person, based on its context within the dataset.   \n",
    "   \n",
    "     \n",
    "      \n",
    "-->   \n",
    "`As a classifier of personal data in tabular datasets, your task is to analyze the provided columns (each containing up to ten distinct values)`  \n",
    "`and determine whether they correspond to a specified semantic data class. `  \n",
    "`If a column matches exactly one of the semantic data classes, return the matching class.`  \n",
    "`Additionally, assess whether the column contains personal information.`  \n",
    "`Classifying semantic data classes is essential for identifying personal information within tabular data. `  \n",
    "`Detecting personal information helps ensure compliance with data protection regulations and safeguards individuals' privacy and security.`  \n",
    "`Output your results in a dictionary format with two keys:`  \n",
    "  \n",
    "`detected_classes: A list of all identified semantic data classes.`  \n",
    "`personal: A boolean indicating whether the column contains data that relates to a natural person, based on its context within the dataset.`  \n",
    "  \n",
    "- Provide one or more example like in Chinese paper:      \n",
    "   \n",
    "`You can use the following examples as a guideline:`  \n",
    "`Example Question : first_name_en_10: ['Tom', 'Walter', 'Mia', 'Lena', 'John', Jack', 'Felice', 'Anna', 'Lukas', 'Will']`  \n",
    "`Does this column correspond perfectly to one of these semantic data classes: [address, credit card number, first name]?`  \n",
    "`Additionally, assess whether the column contains data related to a natural person.`  \n",
    "`Example Question 1: color_de_2: ['blau', 'grün', 'rot', 'gelb', 'schwarz', 'weiß', 'orange', 'lila', 'rosa', 'braun']`  \n",
    "`Does this column correspond perfectly to one of these semantic data classes: [address, credit card number, first name]?`  \n",
    "`Additionally, assess whether the column contains data related to a natural person.`  \n",
    "  \n",
    "`Example Answer 1: {detected_classes: [first_name], personal_related: true}\\nExample Answer 2: {detected_classes: [], personal_related: false}`  \n",
    "  \n",
    "- Sent in the data columns with the SENT method of the paper (seperately instead of all at once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset                   \n",
       "Marvel_Movies_Dataset         18\n",
       "Avocado-Prices-(Augmented)    14\n",
       "vowel                         13\n",
       "forest_fires                  13\n",
       "FOREX_chfjpy-minute-Close     12\n",
       "wine_quality                  12\n",
       "nyc-taxi-green-dec-2016       11\n",
       "echoMonths                    10\n",
       "fishcatch                      8\n",
       "iris                           5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openml_2_dataset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........DESSI-MF........\n",
      "Number DeSSI-MF of Features:546\n",
      "......Kaggle......\n",
      "Number Kaggle of Features:246\n",
      "......Test Languages......\n",
      "Number Test Languages of Features:100\n",
      "......openml_data......\n",
      "Number openml_data of Features:142\n",
      "......openml_2_data......\n",
      "Number openml_2_data of Features:116\n"
     ]
    }
   ],
   "source": [
    "print('........DESSI-MF........')\n",
    "print(f'Number DeSSI-MF of Features:{df_llm.columns.nunique()}')\n",
    "print('......Kaggle......')\n",
    "print(f'Number Kaggle of Features:{kaggle_data.columns.nunique()}')\n",
    "print('......Test Languages......')\n",
    "print(f'Number Test Languages of Features:{test_languages_data.columns.nunique()}')\n",
    "print('......openml_data......')\n",
    "print(f'Number openml_data of Features:{openml_data.columns.nunique()}')\n",
    "print('......openml_2_data......')\n",
    "print(f'Number openml_2_data of Features:{openml_2_data.columns.nunique()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......Kaggle......\n",
      "Number Kaggle sources:\n",
      "dataset            \n",
      "student_performance    33\n",
      "house_price            28\n",
      "absenteeism            21\n",
      "bank_marketing         21\n",
      "indian_companies       17\n",
      "diabetes               17\n",
      "heart_disease          16\n",
      "pixar                  16\n",
      "adult_census           15\n",
      "titanic                12\n",
      "agriculture            12\n",
      "used_car               11\n",
      "indian_liver           11\n",
      "graduate_admissions     9\n",
      "phishing_email          7\n",
      "Name: count, dtype: int64\n",
      " .............................\n",
      "Number openml_data sources:\n",
      "dataset                              \n",
      "TVS_Loan_Default                         32\n",
      "APL_20_24                                27\n",
      "Amazon_Prime_Fiction                     19\n",
      "CSM                                      13\n",
      "mango_detection_australia                12\n",
      "DATASETBANK                              12\n",
      "company_quality_and_valuation_finance    11\n",
      "HousingPrices                             8\n",
      "Oilst_Customers_Dataset                   5\n",
      "FitBit_HeartRate                          3\n",
      "Name: count, dtype: int64\n",
      " .............................\n",
      "......openml_2_data......\n",
      "Number openml_2_data sources:\n",
      "dataset                   \n",
      "Marvel_Movies_Dataset         18\n",
      "Avocado-Prices-(Augmented)    14\n",
      "vowel                         13\n",
      "forest_fires                  13\n",
      "FOREX_chfjpy-minute-Close     12\n",
      "wine_quality                  12\n",
      "nyc-taxi-green-dec-2016       11\n",
      "echoMonths                    10\n",
      "fishcatch                      8\n",
      "iris                           5\n",
      "Name: count, dtype: int64\n",
      " .............................\n"
     ]
    }
   ],
   "source": [
    "print('......Kaggle......')\n",
    "print(f'Number Kaggle sources:\\n{kaggle_dataset.value_counts()}\\n .............................')\n",
    "print(f'Number openml_data sources:\\n{openml_dataset.value_counts()}\\n .............................')\n",
    "print('......openml_2_data......')\n",
    "print(f'Number openml_2_data sources:\\n{openml_2_dataset.value_counts()}\\n .............................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label       \n",
       "personal        282\n",
       "non-personal    264\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_llm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........DESSI-MF....\n",
      "label       \n",
      "personal        282\n",
      "non-personal    264\n",
      "Name: count, dtype: int64\n",
      "(100, 546)\n",
      "....Test Languages ....\n",
      "label       \n",
      "personal        64\n",
      "non-personal    36\n",
      "Name: count, dtype: int64\n",
      "shape: (100, 100)\n",
      "........Kaggle....\n",
      "label       \n",
      "personal        155\n",
      "non-personal     91\n",
      "Name: count, dtype: int64\n",
      "shape: (100, 246)\n",
      ".......openml1....\n",
      "label       \n",
      "non-personal    72\n",
      "personal        70\n",
      "Name: count, dtype: int64\n",
      "shape: (100, 142)\n",
      ".......openml2....\n",
      "label       \n",
      "non-personal    104\n",
      "personal         12\n",
      "Name: count, dtype: int64\n",
      "shape: (100, 116)\n",
      ".......openmall....\n",
      "label       \n",
      "non-personal    176\n",
      "personal         82\n",
      "Name: count, dtype: int64\n",
      "shape: (100, 258)\n"
     ]
    }
   ],
   "source": [
    "print('........DESSI-MF....')\n",
    "print(f'{labels_llm.value_counts()}')\n",
    "print(f'{df_llm.shape}')\n",
    "print('....Test Languages ....')\n",
    "print(f'{test_languages_labels.value_counts()}')\n",
    "print(f'shape: {test_languages_data.shape}')\n",
    "print('........Kaggle....')\n",
    "print(f'{kaggle_labels.value_counts()}')\n",
    "print(f'shape: {kaggle_data.shape}')\n",
    "print('.......openml1....')\n",
    "print(f'{openml_labels.value_counts()}')\n",
    "print(f'shape: {openml_data.shape}')\n",
    "print('.......openml2....')\n",
    "print(f'{openml_2_labels.value_counts()}')\n",
    "print(f'shape: {openml_2_data.shape}')\n",
    "\n",
    "print('.......openmall....')\n",
    "print(f'{openmlall_labels.value_counts()}')\n",
    "print(f'shape: {openmlall.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on own dataset - Dessi-MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"\"\"\n",
    "As a classifier of semantic data classes in tabular datasets, your task is to analyze the provided columns (each containing up to ten distinct values)\n",
    "and determine whether they correspond to a specified semantic data class. \n",
    "If a column matches exactly one of the semantic data classes, return the matching class.\n",
    "Classifying semantic data classes is essential for identifying personal information within tabular data. \n",
    "Detecting personal information helps ensure compliance with data protection regulations and safeguards individuals' privacy and security.\n",
    "Output your results in a dictionary format:\n",
    "\n",
    "detected_classes: A list of all identified semantic data classes.\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = \"\"\"\n",
    "You can use the following examples as a guideline:\n",
    "Example Question 1: first_name_en_10: ['Tom', 'Walter', 'Mia', 'Lena', 'John', 'Jack', 'Felice', 'Anna', 'Lukas', 'Will']\n",
    "Does this column correspond perfectly to one or more of the following semantic data classes: [address, credit card number, first name]?\n",
    "Example Question 2: color_de_2: ['blau', 'grün', 'rot', 'gelb', 'schwarz', 'weiß', 'orange', 'lila', 'rosa', 'braun']\n",
    "Does this column correspond perfectly to one or more of the following semantic data classes: [address, credit card number, first name]?\n",
    "\"\"\"\n",
    "\n",
    "example_answer = \"Example Answer 1: {detected_classes: [first_name]}\\nExample Answer 2: {detected_classes: []}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_classes = sorted(set([a for a in multiclass_llm[\"label\"] if \",\" not in a]))\n",
    "NON_PERSONAL = [\"answer\", \"city\", \"color\", \"company\", \"cpu\", \"credit_card_provider\", \"currency\", \"date\", \"dish\", \"drink\", \n",
    "                \"duration\", \"EAN_code\", \"float_number\", \"gpe\", \"graphics\", \"integer_number\", \"isbn\", \"manufacturer\", \"measure_unit\", \"phone_model\", \n",
    "                \"programming_language\", \"resolution\", \"SWIFT/BIC code\", \"system_quality_attribute\", \"url\", \"user_agent\", \"version\", \"word\"]\n",
    "personal_classes = list(set(semantic_classes) - set(NON_PERSONAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#personal_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Does this column correspond perfectly to one or more of the following semantic data classes: ['EAN_code', 'SWIFT/BIC code', 'academic_degree/title', 'answer', 'blood_group', 'city', 'color', 'company', 'cpu', 'credit_card_number', 'credit_card_provider', 'currency', 'date', 'dish', 'drink', 'duration', 'email', 'first_name', 'float_number', 'full_address', 'full_name', 'gender', 'gpe', 'graphics', 'iban', 'id_card', 'integer_number', 'isbn', 'job', 'language', 'last_name', 'longitude_and_latitude', 'manufacturer', 'measure_unit', 'national_identification_number', 'nationality', 'passport_number', 'phone_model', 'phone_number', 'political_views', 'programming_language', 'race', 'religion/worldview', 'resolution', 'sexuality', 'system_quality_attribute', 'url', 'user_agent', 'version', 'word']?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_prompt = \"Does this column correspond perfectly to one or more of the following semantic data classes: \"  + str(semantic_classes) + \"?\" \n",
    "classification_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the improved version GPT4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction results DESSIMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        uncomment the line1 on the key api before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import gpt\n",
    "gpt.load_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, model: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_token_count = pd.DataFrame()\n",
    "\n",
    "# for i in range(df_llm.shape[1]):\n",
    "#     conversation = [\n",
    "#         {\"role\": \"system\", \"content\": initial_prompt},\n",
    "#         {\"role\": \"user\", \"content\": example_prompt},\n",
    "#         {\"role\": \"assistant\", \"content\": example_answer}\n",
    "#     ]\n",
    "#     metadata = {\n",
    "#     \"Dataset Name\": \"Own Dataset\",\n",
    "#     \"Description\": \"\"\" The dataset was compiled from various sources and contains columns with personal and non-personal data. \n",
    "#     Several columns were synthetically generated using Mimesis and Faker. \n",
    "#     The rest originates from the DeSSI (Dataset for Structured Sensitive Information) dataset.\n",
    "#     \"\"\"\n",
    "#     }\n",
    "#     data_prompt = \"Classify the following column with careful consideration of the dataset description:\\n\" + str(metadata) + \"\\n\"\n",
    "    \n",
    "#     s = \"\\n Column of the dataset to classify: '\" + df_llm.columns[i] + \"': \"\n",
    "#     val_list = df_llm.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "#     s += str(val_list) + \"\\n\"\n",
    "\n",
    "#     data_prompt = data_prompt + s + classification_prompt\n",
    "        \n",
    "#     conversation.append({\"role\": \"user\", \"content\": data_prompt})\n",
    "\n",
    "#     for c in conversation:\n",
    "#         prompt1 = num_tokens_from_string(c['content'], \"gpt-4o\") # system\n",
    "#         prompt2 = num_tokens_from_string(c['content'], \"gpt-4o\") # user\n",
    "#         prompt3 = num_tokens_from_string(c['content'], \"gpt-4o\") # assistant\n",
    "#         df_token_count.loc[i, 'token_count'] = prompt1 + prompt2 + prompt3\n",
    "\n",
    "# df_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1074.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1035.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1095.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>930.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>1038.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>1026.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     token_count\n",
       "0         1074.0\n",
       "1         1167.0\n",
       "2         1035.0\n",
       "3         1095.0\n",
       "4         1104.0\n",
       "..           ...\n",
       "541       1065.0\n",
       "542       1128.0\n",
       "543        930.0\n",
       "544       1038.0\n",
       "545       1026.0\n",
       "\n",
       "[546 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token_count = pd.DataFrame()\n",
    "for i in range(df_llm.shape[1]):\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": initial_prompt},\n",
    "        {\"role\": \"user\", \"content\": example_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": example_answer}\n",
    "    ]\n",
    "    metadata = {\n",
    "    \"Dataset Name\": \"Own Dataset\",\n",
    "    \"Description\": \"\"\" The dataset was compiled from various sources and contains columns with personal and non-personal data. \n",
    "    Several columns were synthetically generated using Mimesis and Faker. \n",
    "    The rest originates from the DeSSI (Dataset for Structured Sensitive Information) dataset.\n",
    "    \"\"\"\n",
    "    }\n",
    "    data_prompt = \"Classify the following column with careful consideration of the dataset description:\\n\" + str(metadata) + \"\\n\"\n",
    "    \n",
    "    s = \"\\n Column of the dataset to classify: '\" + df_llm.columns[i] + \"': \"\n",
    "    val_list = df_llm.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "    s += str(val_list) + \"\\n\"\n",
    "\n",
    "    data_prompt = data_prompt + s + classification_prompt\n",
    "        \n",
    "    conversation.append({\"role\": \"user\", \"content\": data_prompt})\n",
    "\n",
    "    for c in conversation:\n",
    "        prompt1 = num_tokens_from_string(c['content'], \"gpt-4o\") # system\n",
    "        prompt2 = num_tokens_from_string(c['content'], \"gpt-4o\") # user\n",
    "        prompt3 = num_tokens_from_string(c['content'], \"gpt-4o\") # assistant\n",
    "        df_token_count.loc[i, 'token_count'] = prompt1 + prompt2 + prompt3\n",
    "\n",
    "\n",
    "\n",
    "    gpt.add_batch('batchdessimf07.jsonl', conversation) # Updaqte the name of the jsonl\n",
    "    \n",
    "    # response = openai.chat.completions.create(\n",
    "    #     model=\"gpt-4o\",\n",
    "    #     messages=conversation,\n",
    "    #     seed=42\n",
    "    # )\n",
    "    \n",
    "    # with open(\"gpt_predictions/dessi-mf_results.txt\", \"a\") as file:\n",
    "    #     file.write(\"\\n\" + response.choices[0].message.content.replace(\"\\n\", \"\"))\n",
    "\n",
    "df_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77 $\n"
     ]
    }
   ],
   "source": [
    "print(str(round((df_token_count['token_count'].sum()/1000000)*1.25, 2)) + ' $')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch was created with id: batch_67c8578aaca881908ed90b3eff04ca50\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Status: completedg, Elapsed Time: 0 Hours 2 Minutes 51 Secondss\n",
      "Outputfile: file-9YjT5XaGHgC5Dgjr4s188g\n",
      "\n",
      "Writing to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "result = gpt.ask_gpt(batchfile='batchdessimf07.jsonl')\n",
    "result_formatted = []\n",
    "\n",
    "for r in result:\n",
    "    start = r.index(\"[\") + 1\n",
    "    end = r.index(\"]\")\n",
    "    # Extract the substring\n",
    "    res = r[start:end]\n",
    "    res = res.replace(\"'\", '')\n",
    "    res = res.replace('\"', '')\n",
    "    # if res == None:\n",
    "    #     res = ''\n",
    "    result_formatted.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drink',\n",
       " 'date',\n",
       " 'city',\n",
       " 'credit_card_provider',\n",
       " 'political_views',\n",
       " 'resolution',\n",
       " 'integer_number',\n",
       " 'dish',\n",
       " 'gender',\n",
       " 'iban',\n",
       " 'credit_card_provider',\n",
       " 'phone_number',\n",
       " 'company',\n",
       " 'phone_model',\n",
       " 'system_quality_attribute',\n",
       " 'religion/worldview',\n",
       " 'religion/worldview',\n",
       " '',\n",
       " 'credit_card_number',\n",
       " 'iban',\n",
       " 'cpu',\n",
       " 'integer_number',\n",
       " 'answer',\n",
       " 'blood_group',\n",
       " 'job',\n",
       " 'credit_card_number',\n",
       " 'color',\n",
       " 'full_name',\n",
       " 'political_views',\n",
       " 'blood_group',\n",
       " 'last_name',\n",
       " 'full_name',\n",
       " 'national_identification_number',\n",
       " 'graphics',\n",
       " 'manufacturer',\n",
       " 'phone_number',\n",
       " 'last_name',\n",
       " 'company',\n",
       " 'credit_card_provider',\n",
       " 'date, national_identification_number',\n",
       " 'word',\n",
       " 'duration',\n",
       " 'date',\n",
       " 'float_number',\n",
       " 'full_name, academic_degree/title',\n",
       " 'nationality',\n",
       " 'longitude_and_latitude',\n",
       " 'programming_language',\n",
       " '',\n",
       " '',\n",
       " 'measure_unit',\n",
       " 'resolution',\n",
       " 'email',\n",
       " 'duration',\n",
       " 'user_agent',\n",
       " 'full_name',\n",
       " 'academic_degree/title',\n",
       " 'dish',\n",
       " 'full_address',\n",
       " 'passport_number',\n",
       " 'currency',\n",
       " 'date',\n",
       " 'blood_group',\n",
       " 'religion/worldview',\n",
       " 'url',\n",
       " 'gender',\n",
       " 'cpu',\n",
       " 'full_name',\n",
       " 'manufacturer',\n",
       " 'national_identification_number',\n",
       " 'last_name',\n",
       " 'company',\n",
       " 'SWIFT/BIC code',\n",
       " 'longitude_and_latitude',\n",
       " 'nationality',\n",
       " 'graphics',\n",
       " 'color',\n",
       " 'blood_group',\n",
       " 'blood_group',\n",
       " 'gpe',\n",
       " 'credit_card_number',\n",
       " 'dish',\n",
       " 'isbn',\n",
       " 'phone_number',\n",
       " 'SWIFT/BIC code',\n",
       " 'date',\n",
       " 'phone_model',\n",
       " 'job',\n",
       " 'integer_number',\n",
       " 'political_views',\n",
       " 'gender',\n",
       " 'integer_number',\n",
       " 'measure_unit',\n",
       " 'passport_number',\n",
       " 'credit_card_number',\n",
       " 'email',\n",
       " 'float_number',\n",
       " 'integer_number',\n",
       " 'isbn',\n",
       " 'company',\n",
       " 'SWIFT/BIC code',\n",
       " 'email',\n",
       " 'resolution',\n",
       " 'full_address',\n",
       " 'float_number',\n",
       " 'user_agent',\n",
       " 'gender',\n",
       " 'SWIFT/BIC code',\n",
       " 'last_name',\n",
       " 'full_address',\n",
       " 'phone_number',\n",
       " 'float_number',\n",
       " 'full_address',\n",
       " 'duration',\n",
       " 'national_identification_number',\n",
       " 'email',\n",
       " 'duration',\n",
       " 'system_quality_attribute',\n",
       " 'city',\n",
       " 'measure_unit',\n",
       " 'credit_card_number',\n",
       " 'job',\n",
       " 'phone_number',\n",
       " 'isbn',\n",
       " 'date',\n",
       " 'resolution',\n",
       " 'currency',\n",
       " 'full_name',\n",
       " 'blood_group',\n",
       " 'national_identification_number, email',\n",
       " 'first_name',\n",
       " 'job',\n",
       " 'currency',\n",
       " 'url',\n",
       " 'nationality',\n",
       " 'city',\n",
       " 'programming_language',\n",
       " 'academic_degree/title',\n",
       " 'full_address',\n",
       " 'email, national_identification_number, passport_number, phone_number',\n",
       " 'full_address',\n",
       " 'user_agent',\n",
       " 'full_name',\n",
       " 'integer_number',\n",
       " 'measure_unit',\n",
       " 'user_agent',\n",
       " 'email',\n",
       " 'last_name',\n",
       " 'currency',\n",
       " 'color',\n",
       " 'drink',\n",
       " 'blood_group',\n",
       " 'longitude_and_latitude',\n",
       " 'first_name',\n",
       " 'religion/worldview',\n",
       " 'float_number',\n",
       " 'color',\n",
       " '',\n",
       " 'email, phone_number',\n",
       " 'credit_card_provider',\n",
       " 'gender',\n",
       " 'phone_model',\n",
       " 'full_name, academic_degree/title',\n",
       " 'last_name',\n",
       " 'academic_degree/title',\n",
       " 'graphics',\n",
       " 'phone_number',\n",
       " 'measure_unit',\n",
       " 'phone_number',\n",
       " 'first_name',\n",
       " 'color',\n",
       " 'integer_number',\n",
       " 'currency',\n",
       " 'url',\n",
       " '',\n",
       " 'credit_card_number',\n",
       " 'full_name',\n",
       " 'float_number',\n",
       " 'national_identification_number',\n",
       " 'credit_card_number',\n",
       " 'system_quality_attribute',\n",
       " 'date',\n",
       " 'float_number',\n",
       " 'integer_number',\n",
       " 'longitude_and_latitude',\n",
       " 'national_identification_number',\n",
       " 'integer_number',\n",
       " 'blood_group',\n",
       " 'measure_unit',\n",
       " 'company',\n",
       " 'company',\n",
       " 'email',\n",
       " 'last_name',\n",
       " 'phone_number',\n",
       " 'academic_degree/title',\n",
       " 'credit_card_number',\n",
       " 'first_name',\n",
       " 'graphics',\n",
       " 'nationality',\n",
       " 'credit_card_number',\n",
       " 'gender',\n",
       " 'currency',\n",
       " 'language',\n",
       " '',\n",
       " '',\n",
       " 'company',\n",
       " 'graphics',\n",
       " 'phone_number',\n",
       " 'EAN_code',\n",
       " 'full_name',\n",
       " 'last_name',\n",
       " 'academic_degree/title',\n",
       " 'credit_card_provider',\n",
       " 'integer_number',\n",
       " 'email',\n",
       " 'job',\n",
       " '',\n",
       " 'religion/worldview',\n",
       " 'credit_card_number',\n",
       " 'company',\n",
       " '',\n",
       " 'SWIFT/BIC code',\n",
       " 'cpu',\n",
       " 'first_name',\n",
       " 'blood_group',\n",
       " 'full_address',\n",
       " 'academic_degree/title',\n",
       " 'color',\n",
       " 'phone_model',\n",
       " 'manufacturer',\n",
       " 'religion/worldview',\n",
       " 'dish',\n",
       " 'email',\n",
       " 'nationality',\n",
       " 'cpu',\n",
       " 'integer_number',\n",
       " 'dish',\n",
       " 'float_number',\n",
       " 'first_name',\n",
       " 'national_identification_number',\n",
       " 'credit_card_number',\n",
       " 'passport_number',\n",
       " 'race',\n",
       " 'language',\n",
       " 'phone_number',\n",
       " 'longitude_and_latitude',\n",
       " 'full_address',\n",
       " 'job',\n",
       " '',\n",
       " 'color',\n",
       " 'phone_number',\n",
       " 'full_address',\n",
       " 'answer',\n",
       " 'job',\n",
       " 'system_quality_attribute',\n",
       " 'EAN_code',\n",
       " 'programming_language',\n",
       " 'full_address',\n",
       " 'manufacturer',\n",
       " 'passport_number',\n",
       " 'graphics',\n",
       " 'phone_number',\n",
       " 'email',\n",
       " 'last_name',\n",
       " 'system_quality_attribute',\n",
       " 'sexuality',\n",
       " 'full_name',\n",
       " 'religion/worldview',\n",
       " 'iban',\n",
       " 'version',\n",
       " 'float_number',\n",
       " 'full_name',\n",
       " 'phone_number, national_identification_number',\n",
       " 'national_identification_number',\n",
       " 'job',\n",
       " 'credit_card_number',\n",
       " '',\n",
       " 'isbn',\n",
       " 'company',\n",
       " 'job',\n",
       " 'iban',\n",
       " 'last_name',\n",
       " 'iban',\n",
       " 'gender',\n",
       " 'company',\n",
       " 'email',\n",
       " 'gender',\n",
       " 'user_agent',\n",
       " 'float_number',\n",
       " 'iban',\n",
       " 'blood_group',\n",
       " '',\n",
       " 'blood_group',\n",
       " '',\n",
       " '',\n",
       " 'company',\n",
       " 'manufacturer',\n",
       " 'word',\n",
       " 'integer_number',\n",
       " 'national_identification_number',\n",
       " 'company',\n",
       " 'word',\n",
       " 'nationality',\n",
       " 'job',\n",
       " 'full_name',\n",
       " 'company',\n",
       " '',\n",
       " 'first_name',\n",
       " 'manufacturer',\n",
       " 'isbn',\n",
       " 'job',\n",
       " 'programming_language',\n",
       " 'drink',\n",
       " 'nationality',\n",
       " 'version',\n",
       " 'url',\n",
       " 'phone_number, email',\n",
       " 'first_name',\n",
       " 'resolution',\n",
       " 'blood_group',\n",
       " 'color',\n",
       " 'credit_card_number',\n",
       " 'duration',\n",
       " 'city',\n",
       " 'float_number',\n",
       " 'religion/worldview',\n",
       " 'programming_language',\n",
       " 'first_name',\n",
       " 'blood_group',\n",
       " 'language',\n",
       " 'word',\n",
       " 'language',\n",
       " 'phone_number',\n",
       " 'credit_card_number',\n",
       " 'phone_number',\n",
       " 'user_agent',\n",
       " 'blood_group',\n",
       " 'cpu',\n",
       " 'gender',\n",
       " 'job',\n",
       " 'last_name',\n",
       " 'isbn',\n",
       " 'date',\n",
       " 'date',\n",
       " 'drink',\n",
       " 'programming_language',\n",
       " 'color',\n",
       " 'first_name',\n",
       " 'last_name',\n",
       " 'SWIFT/BIC code',\n",
       " 'language',\n",
       " 'answer',\n",
       " 'blood_group',\n",
       " 'dish',\n",
       " 'full_address',\n",
       " 'isbn',\n",
       " 'academic_degree/title',\n",
       " 'version',\n",
       " 'email',\n",
       " 'political_views',\n",
       " 'first_name',\n",
       " 'phone_model',\n",
       " 'company',\n",
       " 'full_address',\n",
       " 'job',\n",
       " 'user_agent',\n",
       " 'last_name',\n",
       " 'credit_card_number',\n",
       " 'credit_card_number',\n",
       " 'phone_number',\n",
       " 'gender',\n",
       " 'gender',\n",
       " 'word',\n",
       " 'language',\n",
       " 'word',\n",
       " 'SWIFT/BIC code',\n",
       " 'currency',\n",
       " 'EAN_code',\n",
       " 'cpu',\n",
       " 'integer_number',\n",
       " 'last_name',\n",
       " 'full_name',\n",
       " 'float_number',\n",
       " 'float_number',\n",
       " 'email',\n",
       " 'measure_unit',\n",
       " 'phone_model',\n",
       " 'answer',\n",
       " 'company',\n",
       " 'credit_card_number',\n",
       " 'longitude_and_latitude',\n",
       " 'longitude_and_latitude',\n",
       " 'duration',\n",
       " 'isbn',\n",
       " 'url',\n",
       " 'full_name',\n",
       " 'phone_number',\n",
       " 'answer',\n",
       " 'answer',\n",
       " 'nationality',\n",
       " 'first_name',\n",
       " 'company',\n",
       " 'system_quality_attribute',\n",
       " 'resolution',\n",
       " 'isbn',\n",
       " 'national_identification_number, date',\n",
       " 'manufacturer',\n",
       " 'isbn',\n",
       " 'version',\n",
       " 'cpu',\n",
       " 'phone_number',\n",
       " 'job',\n",
       " 'academic_degree/title',\n",
       " 'language',\n",
       " 'national_identification_number, phone_number',\n",
       " 'full_address',\n",
       " 'date',\n",
       " 'dish',\n",
       " 'company',\n",
       " 'email',\n",
       " 'city',\n",
       " 'isbn',\n",
       " 'academic_degree/title',\n",
       " 'iban',\n",
       " 'academic_degree/title',\n",
       " 'system_quality_attribute',\n",
       " 'city',\n",
       " 'email',\n",
       " 'email',\n",
       " 'integer_number',\n",
       " 'date',\n",
       " 'gender',\n",
       " 'job',\n",
       " 'credit_card_number',\n",
       " 'color',\n",
       " '',\n",
       " 'nationality',\n",
       " 'academic_degree/title',\n",
       " 'email',\n",
       " 'drink',\n",
       " 'EAN_code',\n",
       " 'email, national_identification_number',\n",
       " 'float_number',\n",
       " 'religion/worldview',\n",
       " 'longitude_and_latitude',\n",
       " 'city',\n",
       " 'word',\n",
       " 'email',\n",
       " 'url',\n",
       " 'drink',\n",
       " 'EAN_code',\n",
       " 'duration',\n",
       " 'full_address',\n",
       " 'job',\n",
       " 'national_identification_number',\n",
       " 'url',\n",
       " 'isbn',\n",
       " 'blood_group',\n",
       " 'version',\n",
       " 'national_identification_number',\n",
       " '',\n",
       " 'city',\n",
       " 'iban',\n",
       " 'resolution',\n",
       " 'integer_number',\n",
       " 'answer',\n",
       " 'gender',\n",
       " 'iban',\n",
       " 'city',\n",
       " 'academic_degree/title',\n",
       " 'email',\n",
       " 'url',\n",
       " 'programming_language',\n",
       " 'political_views',\n",
       " 'gender',\n",
       " 'first_name',\n",
       " 'isbn',\n",
       " 'credit_card_provider',\n",
       " 'nationality',\n",
       " 'integer_number',\n",
       " 'phone_model',\n",
       " 'answer',\n",
       " 'first_name',\n",
       " 'email',\n",
       " 'full_address',\n",
       " 'graphics',\n",
       " 'currency',\n",
       " 'float_number',\n",
       " 'last_name',\n",
       " 'gender',\n",
       " 'iban',\n",
       " 'isbn',\n",
       " 'duration',\n",
       " 'credit_card_provider',\n",
       " 'sexuality',\n",
       " 'user_agent',\n",
       " 'credit_card_provider',\n",
       " 'drink',\n",
       " 'longitude_and_latitude',\n",
       " 'measure_unit',\n",
       " '',\n",
       " 'cpu',\n",
       " 'credit_card_number',\n",
       " 'full_name',\n",
       " 'credit_card_number',\n",
       " 'academic_degree/title',\n",
       " 'race',\n",
       " 'id_card',\n",
       " 'color',\n",
       " 'dish',\n",
       " 'graphics',\n",
       " 'system_quality_attribute',\n",
       " 'programming_language',\n",
       " 'gender',\n",
       " '',\n",
       " 'first_name',\n",
       " 'resolution',\n",
       " 'isbn',\n",
       " 'first_name',\n",
       " 'EAN_code',\n",
       " 'political_views',\n",
       " 'academic_degree/title',\n",
       " '',\n",
       " 'academic_degree/title',\n",
       " 'drink',\n",
       " 'political_views',\n",
       " 'word',\n",
       " 'full_address',\n",
       " 'last_name',\n",
       " 'email, phone_number',\n",
       " 'gender',\n",
       " 'float_number',\n",
       " 'phone_number',\n",
       " 'isbn',\n",
       " 'phone_number',\n",
       " 'passport_number',\n",
       " 'national_identification_number, email',\n",
       " 'academic_degree/title',\n",
       " 'longitude_and_latitude',\n",
       " 'language',\n",
       " 'political_views',\n",
       " 'last_name',\n",
       " 'phone_model',\n",
       " 'gender',\n",
       " 'religion/worldview',\n",
       " 'manufacturer']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>batch_gpt_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drink_mixed_mimesis</td>\n",
       "      <td>drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MnTvnRO2qGFq56</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>city_en_mimesis</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>credit_card_provider_de_DE_faker</td>\n",
       "      <td>credit_card_provider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48i2GDrmZhvkUDPqTlaV</td>\n",
       "      <td>political_views</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>Gqi7w4e4pxs</td>\n",
       "      <td>last_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>phone_model_fr_mimesis</td>\n",
       "      <td>phone_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>sex_en_faker</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>XKANObF4OIs</td>\n",
       "      <td>religion/worldview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>manufacturer_mixed_mimesis</td>\n",
       "      <td>manufacturer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Feature  batch_gpt_prediction\n",
       "0                 drink_mixed_mimesis                 drink\n",
       "1                      MnTvnRO2qGFq56                  date\n",
       "2                     city_en_mimesis                  city\n",
       "3    credit_card_provider_de_DE_faker  credit_card_provider\n",
       "4                48i2GDrmZhvkUDPqTlaV       political_views\n",
       "..                                ...                   ...\n",
       "541                       Gqi7w4e4pxs             last_name\n",
       "542            phone_model_fr_mimesis           phone_model\n",
       "543                      sex_en_faker                gender\n",
       "544                       XKANObF4OIs    religion/worldview\n",
       "545        manufacturer_mixed_mimesis          manufacturer\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions = pd.DataFrame()\n",
    "\n",
    "df_predictions['Feature'] = df_llm.columns.tolist()\n",
    "\n",
    "for i, r in enumerate(result_formatted):\n",
    "    df_predictions.loc[i, 'batch_gpt_prediction'] = r\n",
    "\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_predictions.to_csv('dessimf_predictions_batch_gpt03.csv', index=False)\n",
    "df_predictions.to_csv('dessimf_predictions_batch_gpt07.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(546, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on two new languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    token_count\n",
      "0         912.0\n",
      "1        1050.0\n",
      "2         996.0\n",
      "3        1077.0\n",
      "4         990.0\n",
      "..          ...\n",
      "95       1050.0\n",
      "96       1038.0\n",
      "97       1098.0\n",
      "98       1128.0\n",
      "99       1041.0\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_token_count_lan = pd.DataFrame()\n",
    "for i in range(test_languages_data.shape[1]):\n",
    "    conversation1 = [\n",
    "        {\"role\": \"system\", \"content\": initial_prompt},\n",
    "        {\"role\": \"user\", \"content\": example_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": example_answer}\n",
    "    ]\n",
    "    metadata1 = {\n",
    "    \"Dataset Name\": \"Test Languages\",\n",
    "    \"Description\": \"\"\" The dataset was synthetically generated using Mimesis and Faker and contains columns with personal and non-personal data.\n",
    "    The values are in Italian and Chinese.\n",
    "    \"\"\"\n",
    "    }\n",
    "    data_prompt1 = \"Classify the following column with careful consideration of the dataset description:\\n\" + str(metadata1) + \"\\n\"\n",
    "    \n",
    "    s1 = \"\\n Column of the dataset to classify: '\" + test_languages_data.columns[i] + \"': \"\n",
    "    val_list1 = test_languages_data.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "    s1 += str(val_list1) + \"\\n\"\n",
    "\n",
    "    data_prompt1 = data_prompt1 + s1 + classification_prompt\n",
    "        \n",
    "    conversation1.append({\"role\": \"user\", \"content\": data_prompt1})\n",
    "\n",
    "    for c in conversation1:\n",
    "        prompt1l = num_tokens_from_string(c['content'], \"gpt-4o\") # system\n",
    "        prompt2l = num_tokens_from_string(c['content'], \"gpt-4o\") # user\n",
    "        prompt3l = num_tokens_from_string(c['content'], \"gpt-4o\") # assistant\n",
    "        df_token_count_lan.loc[i, 'token_count'] = prompt1l + prompt2l + prompt3l\n",
    "\n",
    "\n",
    "    gpt.add_batch('batchlan07.jsonl', conversation1) # Updaqte the name of the jsonl\n",
    "    \n",
    "    # response = openai.chat.completions.create(\n",
    "    #     model=\"gpt-4o\",\n",
    "    #     messages=conversation,\n",
    "    #     seed=42\n",
    "    # )\n",
    "    \n",
    "    # with open(\"gpt_predictions/test_languages_results.txt\", \"a\") as file:\n",
    "    #     file.write(\"\\n\" + response.choices[0].message.content.replace(\"\\n\", \"\"))\n",
    "\n",
    "print(df_token_count_lan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch was created with id: batch_67c8584eb2e881909f9f2f72abcf69a2\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Status: completedg, Elapsed Time: 0 Hours 1 Minutes 21 Secondss\n",
      "Outputfile: file-7HkyS8gEdEBb1oSvTeSvMQ\n",
      "\n",
      "Writing to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "resultlan = gpt.ask_gpt(batchfile='batchlan07.jsonl')\n",
    "result_formattedlan = []\n",
    "\n",
    "for r in resultlan:\n",
    "    start = r.index(\"[\") + 1\n",
    "    end = r.index(\"]\")\n",
    "    # Extract the substring\n",
    "    res1 = r[start:end]\n",
    "    res1= res1.replace(\"'\", '')\n",
    "    res1 = res1.replace('\"', '')\n",
    "    # if res == None:\n",
    "    #     res = ''\n",
    "    result_formattedlan.append(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>batch_gpt_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academic_degree_it_1</td>\n",
       "      <td>academic_degree/title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fbh7x41Ztpdp4K8</td>\n",
       "      <td>job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first_name_zh_1</td>\n",
       "      <td>first_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>address_it_1</td>\n",
       "      <td>full_address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeTOdoe5c3ve</td>\n",
       "      <td>religion/worldview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>VCJTBJahe8</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phone_number_zh_1</td>\n",
       "      <td>phone_number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ssn_zh_1</td>\n",
       "      <td>national_identification_number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blV7vY7AZQ3VZprkYS</td>\n",
       "      <td>url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>email_zh_1</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Feature            batch_gpt_prediction\n",
       "0   academic_degree_it_1           academic_degree/title\n",
       "1        Fbh7x41Ztpdp4K8                             job\n",
       "2        first_name_zh_1                      first_name\n",
       "3           address_it_1                    full_address\n",
       "4           aeTOdoe5c3ve              religion/worldview\n",
       "..                   ...                             ...\n",
       "95            VCJTBJahe8                           email\n",
       "96     phone_number_zh_1                    phone_number\n",
       "97              ssn_zh_1  national_identification_number\n",
       "98    blV7vY7AZQ3VZprkYS                             url\n",
       "99            email_zh_1                           email\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictionslan = pd.DataFrame()\n",
    "\n",
    "df_predictionslan['Feature'] = test_languages_data.columns.tolist()\n",
    "\n",
    "for i, r in enumerate(result_formattedlan):\n",
    "    df_predictionslan.loc[i, 'batch_gpt_prediction'] = r\n",
    "\n",
    "df_predictionslan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictionslan.to_csv('Lan_predictions_batch_gpt07.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt_kaggle = \"\"\"\n",
    "As a classifier of personal related data in tabular datasets, your task is to analyze the provided columns (each containing up to ten distinct values)\n",
    "and determine whether they contain information that originates from or relates to a person, even if it is not directly identifiable. \n",
    "Detecting personal related information helps ensure compliance with data protection regulations and safeguards individuals' privacy and security.\n",
    "Output your results in a dictionary format with a boolean indicating if the column contains personal related data or not.\n",
    "\"\"\"\n",
    "\n",
    "example_prompt_kaggle = \"\"\"\n",
    "You can use the following example as a guideline:\n",
    "Classify the following column with careful consideration of the dataset description:\n",
    "Dataset: Title: \"Test Dataset\"\n",
    "Description: \"This dataset was used for a linear regression.\"\n",
    "Features: ['first_name_en_10', 'last_name_en_10', 'email_en_10', 'phone_number', 'address_en_10', 'city_en_10', 'country_en_10', 'date', 'target']\n",
    "Column of the dataset to classify: 'first_name_en_10': ['Tom', 'Walter', 'Mia', 'Lena', 'John', 'Jack', 'Felice', 'Anna', 'Lukas', 'Will']\n",
    "Does this column, in the context of the dataset, contain information relating to a natural person?\n",
    "\"\"\"\n",
    "\n",
    "example_answer_kaggle = \"Example Answer: {first_name_en_10: true}\"\n",
    "classification_prompt_kaggle = \"Does this column, in the context of the dataset, contain information relating to a natural person?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     token_count\n",
      "0         1773.0\n",
      "1         1779.0\n",
      "2         1743.0\n",
      "3         1737.0\n",
      "4         1704.0\n",
      "..           ...\n",
      "241       1377.0\n",
      "242       1386.0\n",
      "243       1437.0\n",
      "244       1869.0\n",
      "245       1632.0\n",
      "\n",
      "[246 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_token_count_kag = pd.DataFrame()\n",
    "for i in range(kaggle_data.shape[1]):\n",
    "    conversation2 = [\n",
    "        {\"role\": \"system\", \"content\": initial_prompt_kaggle},\n",
    "        {\"role\": \"user\", \"content\": example_prompt_kaggle},\n",
    "        {\"role\": \"assistant\", \"content\": example_answer_kaggle}\n",
    "    ]\n",
    "    \n",
    "    with open(f\"../datasets/kaggle_datasets/{kaggle_dataset.iloc[i,0]}/dataset-metadata.json\", \"r\") as f:\n",
    "        metadata2 = json.load(f)\n",
    "    data_prompt2 = \"Classify the following column with careful consideration of the dataset description. Dataset: Title: \" + metadata2[\"title\"] + \"\\n\"\n",
    "    data_prompt2 = data_prompt2 + \"Description: \" + metadata2[\"description\"] + \"\\n\"\n",
    "    data_prompt2 = data_prompt2 + \"Features: \" + str(kaggle_data.iloc[:, kaggle_dataset.loc[kaggle_dataset[\"dataset\"] == kaggle_dataset.iloc[i,0]].index].columns)\n",
    "        \n",
    "    s2 = \"\\n Column of the dataset to classify: '\" + kaggle_data.columns[i] + \"': \"\n",
    "    val_list2 = kaggle_data.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "    s2 += str(val_list2) + \"\\n\"\n",
    "\n",
    "    data_prompt2 = data_prompt2 + s2 + classification_prompt_kaggle\n",
    "        \n",
    "    conversation2.append({\"role\": \"user\", \"content\": data_prompt2})\n",
    "\n",
    "    for c in conversation2:\n",
    "        prompt1k = num_tokens_from_string(c['content'], \"gpt-4o\") # system\n",
    "        prompt2k = num_tokens_from_string(c['content'], \"gpt-4o\") # user\n",
    "        prompt3k = num_tokens_from_string(c['content'], \"gpt-4o\") # assistant\n",
    "        df_token_count_kag.loc[i, 'token_count'] = prompt1k + prompt2k + prompt3k\n",
    "\n",
    "\n",
    "    gpt.add_batch('batchkaggle07.jsonl', conversation2) # Updaqte the name of the jsonl\n",
    "    \n",
    "    # response = openai.chat.completions.create(\n",
    "    #     model=\"gpt-4o\",\n",
    "    #     messages=conversation,\n",
    "    #     seed=42\n",
    "    # )\n",
    "    \n",
    "    # with open(\"gpt_predictions/kaggle_results.txt\", \"a\") as file:\n",
    "    #     file.write(\"\\n\" + response.choices[0].message.content.replace(\"\\n\", \"\"))\n",
    "\n",
    "print(df_token_count_kag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Classify the following column with careful consideration of the dataset description. Dataset: Title: Used Car Dataset\\nDescription: ## Used Cars Dataset\\n**Overview**\\nThis dataset contains information about used cars in the Indian market, comprising 9,582 entries with 11 detailed attributes. The data appears to be collected up to November 2024, providing a comprehensive view of the second-hand car market in India.\\n\\n### Dataset Features\\n**Brand:** Car manufacturer (e.g., Volkswagen, Maruti Suzuki, Honda, Tata)  \\n**Model:** Specific car model (e.g., Taigun, Baleno, Polo, WRV)  \\n**Year:** Manufacturing year of the vehicle (ranging from older models to 2024)  \\n**Age:** Age of the vehicle in years  \\n**kmDriven:** Total kilometers driven by the vehicle  \\n**Transmission:** Type of transmission (Manual or Automatic)  \\n**Owner:** Ownership status (first or second owner)  \\n**FuelType:** Type of fuel (Petrol, Diesel, Hybrid/CNG)  \\n**PostedDate:** When the car listing was posted  \\n**AdditionalInfo:** Extra details about the vehicle  \\n**AskPrice:** Listed price in Indian Rupees (₹)  \\n\\n### Dataset Statistics\\n- Total entries: 9,582\\n- Columns: 11\\n- Memory usage: 823.6+ KB\\n- Data types: Mixed (int64 and object)\\n\\n### Potential Use Cases\\n- Price prediction modeling for used cars in India\\n- Market analysis of different car segments\\n- Study of depreciation patterns\\n- Analysis of fuel type preferences\\n- Understanding transmission preferences in the Indian market\\n- Examining the relationship between kilometers driven and pricing\\n- Brand value retention analysis\\n\\nThis dataset would be valuable for data scientists, automotive market analysts, and machine learning practitioners interested in the Indian automotive sector.\\nFeatures: Index(['Brand', 'model', 'Year.1', 'Age.4', 'kmDriven', 'Transmission',\\n       'Owner', 'FuelType', 'PostedDate', 'AdditionInfo', 'AskPrice'],\\n      dtype='object')\\n Column of the dataset to classify: 'AskPrice': ['₹ 4,25,000', '₹ 2,95,000', '₹ 5,75,000', '₹ 3,00,000', '₹ 6,85,000', '₹ 12,50,000', '₹ 11,25,000', '₹ 6,90,000', '₹ 6,10,000', '₹ 4,29,101']\\nDoes this column, in the context of the dataset, contain information relating to a natural person?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch was created with id: batch_67c858b782e4819083446fcfff4dbcbf\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Status: completedss, Elapsed Time: 0 Hours 1 Minutes 21 Seconds\n",
      "Outputfile: file-RrpiaZ9JtWmG9jbG4hGTxH\n",
      "\n",
      "Writing to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "resultkaggle = gpt.ask_gpt(batchfile='batchkaggle07.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def extract_dict_content(s):\n",
    "    try:\n",
    "        # Try to parse as a dictionary\n",
    "        d = ast.literal_eval(s)\n",
    "        if isinstance(d, dict):\n",
    "            return next(iter(d.items()))\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If parsing fails, try to extract using regex\n",
    "        match = re.match(r\"{'(.+?)'\\s*:\\s*(true|false)}\", s, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1), match.group(2).lower() == 'true'\n",
    "    return s, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Feature  batch_gpt_prediction\n",
      "0                    ID                  True\n",
      "1    Reason for absence                  True\n",
      "2      Month of absence                 False\n",
      "3       Day of the week                 False\n",
      "4               Seasons                 False\n",
      "..                  ...                   ...\n",
      "241               Owner                  True\n",
      "242            FuelType                 False\n",
      "243          PostedDate                 False\n",
      "244        AdditionInfo                 False\n",
      "245            AskPrice                 False\n",
      "\n",
      "[246 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Extract valid entries\n",
    "result_formattedkaggle1 = []\n",
    "for r in resultkaggle:\n",
    "    start = r.find(\"[\") + 1\n",
    "    end = r.rfind(\"]\")\n",
    "    if start > 0 and end != -1:\n",
    "        result_formattedkaggle1.append(r[start:end])\n",
    "    else:\n",
    "        result_formattedkaggle1.append(r)\n",
    "\n",
    "# Step 2: Parse and extract feature names + predictions\n",
    "parsed_results = []\n",
    "for r in result_formattedkaggle1:\n",
    "    feature, prediction = extract_dict_content(r.strip())\n",
    "    if prediction is None:\n",
    "        # If not a dictionary, try to split by comma\n",
    "        parts = r.split(',')\n",
    "        if len(parts) == 2:\n",
    "            feature, prediction = parts[0].strip(), parts[1].strip().lower() == 'true'\n",
    "        else:\n",
    "            feature, prediction = r, None\n",
    "    parsed_results.append((feature, prediction))\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "df_predictionskaggle = pd.DataFrame(parsed_results, columns=['Feature', 'batch_gpt_prediction'])\n",
    "\n",
    "# Step 4: Clean up the DataFrame\n",
    "df_predictionskaggle['Feature'] = df_predictionskaggle['Feature'].str.strip()\n",
    "df_predictionskaggle['batch_gpt_prediction'] = df_predictionskaggle['batch_gpt_prediction'].fillna(False)\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "df_predictionskaggle.to_csv('kaggle_predictions_batch_gpt07.csv', index=False)\n",
    "\n",
    "# Step 6: Display DataFrame\n",
    "print(df_predictionskaggle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on openML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Does this column, in the context of the dataset, contain information relating to a natural person?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_prompt_kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open ML1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     token_count\n",
      "0         2679.0\n",
      "1         2727.0\n",
      "2         2799.0\n",
      "3         2736.0\n",
      "4         2832.0\n",
      "..           ...\n",
      "137       2616.0\n",
      "138       2616.0\n",
      "139       2589.0\n",
      "140       2598.0\n",
      "141       2544.0\n",
      "\n",
      "[142 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_token_count_open1 = pd.DataFrame()\n",
    "for i in range(openml_data.shape[1]):\n",
    "    conversation3 = [\n",
    "        {\"role\": \"system\", \"content\": initial_prompt_kaggle},\n",
    "        {\"role\": \"user\", \"content\": example_prompt_kaggle},\n",
    "        {\"role\": \"assistant\", \"content\": example_answer_kaggle}\n",
    "    ]\n",
    "    \n",
    "    with open(f\"../datasets/openml_datasets/{openml_dataset.iloc[i,0]}/metadata.json\", \"r\") as f:\n",
    "        metadata3 = json.load(f)\n",
    "    data_prompt3 = \"Classify the following column with careful consideration of the dataset's description:\\n\" + str(metadata3) + \"\\n\"\n",
    "    \n",
    "    s3 = \"\\n Column of the dataset to classify: '\" + openml_data.columns[i] + \"': \"\n",
    "    val_list3 = openml_data.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "    s3 += str(val_list3) + \"\\n\"\n",
    "\n",
    "    data_prompt3 = data_prompt3 + s3 + classification_prompt_kaggle\n",
    "        \n",
    "    conversation3.append({\"role\": \"user\", \"content\": data_prompt3})\n",
    "\n",
    "    for c in conversation3:\n",
    "        prompt1open1 = num_tokens_from_string(c['content'], \"gpt-4o\") # system\n",
    "        prompt2open1 = num_tokens_from_string(c['content'], \"gpt-4o\") # user\n",
    "        prompt3open1 = num_tokens_from_string(c['content'], \"gpt-4o\") # assistant\n",
    "        df_token_count_open1.loc[i, 'token_count'] = prompt1open1 + prompt2open1 + prompt3open1\n",
    "\n",
    "\n",
    "    gpt.add_batch('batchopen107.jsonl', conversation3) # Updaqte the name of the jsonl\n",
    "    \n",
    "    # response = openai.chat.completions.create(\n",
    "    #     model=\"gpt-4o\",\n",
    "    #     messages=conversation,\n",
    "    #     seed=42\n",
    "    # )\n",
    "    \n",
    "    # with open(\"gpt_predictions/openml_results.txt\", \"a\") as file:\n",
    "    #     file.write(\"\\n\" + response.choices[0].message.content.replace(\"\\n\", \"\"))\n",
    "print(df_token_count_open1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch was created with id: batch_67c859201b888190a27beb2a35eb0038\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Status: completedss, Elapsed Time: 0 Hours 3 Minutes 22 Seconds\n",
      "Outputfile: file-JgW1KoUxPYVDy74FMDadyW\n",
      "\n",
      "Writing to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "resultopen1 = gpt.ask_gpt(batchfile='batchopen107.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Feature  batch_gpt_prediction\n",
      "0          User ID                  True\n",
      "1             Name                  True\n",
      "2    Email Address                  True\n",
      "3         Username                  True\n",
      "4    Date of Birth                  True\n",
      "..             ...                   ...\n",
      "137            V28                  True\n",
      "138            V29                  True\n",
      "139            V30                  True\n",
      "140            V31                  True\n",
      "141            V32                  True\n",
      "\n",
      "[142 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Extract valid entries\n",
    "result_formattedopen1 = []\n",
    "for r in resultopen1:\n",
    "    start = r.find(\"[\") + 1\n",
    "    end = r.rfind(\"]\")\n",
    "    if start > 0 and end != -1:\n",
    "        result_formattedopen1.append(r[start:end])\n",
    "    else:\n",
    "        result_formattedopen1.append(r)\n",
    "\n",
    "# Step 2: Parse and extract feature names + predictions\n",
    "parsed_resultsopen1 = []\n",
    "for r in result_formattedopen1:\n",
    "    feature, prediction = extract_dict_content(r.strip())\n",
    "    if prediction is None:\n",
    "        # If not a dictionary, try to split by comma\n",
    "        parts = r.split(',')\n",
    "        if len(parts) == 2:\n",
    "            feature, prediction = parts[0].strip(), parts[1].strip().lower() == 'true'\n",
    "        else:\n",
    "            feature, prediction = r, None\n",
    "    parsed_resultsopen1.append((feature, prediction))\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "\n",
    "df_predictionsopen1 = pd.DataFrame(parsed_resultsopen1, columns=['Feature', 'batch_gpt_prediction'])\n",
    "\n",
    "# Step 4: Clean up the DataFrame\n",
    "\n",
    "df_predictionsopen1['Feature'] = df_predictionsopen1['Feature'].str.strip()\n",
    "\n",
    "df_predictionsopen1['batch_gpt_prediction'] = df_predictionsopen1['batch_gpt_prediction'].fillna(False)\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "df_predictionsopen1.to_csv('open1_predictions_batch_gpt07.csv', index=False)\n",
    "\n",
    "# Step 6: Display DataFrame\n",
    "print(df_predictionsopen1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpeML2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_count_open2 = pd.DataFrame()\n",
    "for i in range(openml_2_data.shape[1]):\n",
    "    conversation4 = [\n",
    "        {\"role\": \"system\", \"content\": initial_prompt_kaggle},\n",
    "        {\"role\": \"user\", \"content\": example_prompt_kaggle},\n",
    "        {\"role\": \"assistant\", \"content\": example_answer_kaggle}\n",
    "    ]\n",
    "    \n",
    "    with open(f\"../datasets/openml_datasets_2/{openml_2_dataset.iloc[i,0]}/metadata.json\", \"r\") as f:\n",
    "        metadata4 = json.load(f)\n",
    "    data_prompt4 = \"Classify the following column with careful consideration of the dataset's description:\\n\" + str(metadata4) + \"\\n\"\n",
    "\n",
    "    s4 = \"\\n Column of the dataset to classify: '\" + openml_2_data.columns[i] + \"': \"\n",
    "    val_list4 = openml_2_data.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "    s4 += str(val_list4) + \"\\n\"\n",
    "\n",
    "    data_prompt4 = data_prompt4 + s4 + classification_prompt_kaggle\n",
    "        \n",
    "    conversation4.append({\"role\": \"user\", \"content\": data_prompt4})\n",
    "\n",
    "    for c in conversation4:\n",
    "        prompt1open2 = num_tokens_from_string(c['content'], \"gpt-4o\") # system\n",
    "        prompt2open2 = num_tokens_from_string(c['content'], \"gpt-4o\") # user\n",
    "        prompt3open2 = num_tokens_from_string(c['content'], \"gpt-4o\") # assistant\n",
    "        df_token_count_open2.loc[i, 'token_count'] = prompt1open2 + prompt2open2 + prompt3open2\n",
    "\n",
    "\n",
    "    gpt.add_batch('batchopen207.jsonl', conversation4) # Updaqte the name of the jsonl\n",
    "    \n",
    "    # response = openai.chat.completions.create(\n",
    "    #     model=\"gpt-4o\",\n",
    "    #     messages=conversation,\n",
    "    #     seed=42\n",
    "    # )\n",
    "    \n",
    "    # with open(\"gpt_predictions/openml_2_results.txt\", \"a\") as file:\n",
    "    #     file.write(\"\\n\" + response.choices[0].message.content.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classify the following column with careful consideration of the dataset\\'s description:\\n{\\'Dataset Name: \\': \\'wine_quality\\', \\'Description: \\': \\'**Author**: Tobias Kuehn  \\\\n**Source**: Unknown - 2009  \\\\n**Please cite**:   \\\\n\\\\n1. Title: Wine Quality \\\\n\\\\n2. Sources\\\\nCreated by: Paulo Cortez (Univ. Minho), Antonio Cerdeira, Fernando Almeida, Telmo Matos and Jose Reis (CVRVV) @ 2009\\\\n    \\\\n3. Past Usage:\\\\nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \\\\nModeling wine preferences by data mining from physicochemical properties.\\\\nIn Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\\\\n\\\\nIn the above reference, two datasets were created, using red and white wine samples.\\\\nThe inputs include objective tests (e.g. PH values) and the output is based on sensory data (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality between 0 (very bad) and 10 (very excellent). Several data mining methods were applied to model these datasets under a regression approach. The support vector machine model achieved the best results. Several metrics were computed: MAD, confusion matrix for a fixed error tolerance (T), etc. Also, we plot the relative importances of the input variables (as measured by a sensitivity analysis procedure).\\\\n \\\\n4. Relevant Information:\\\\nThe two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables  are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\\\\nThese datasets can be viewed as classification or regression tasks.\\\\nThe classes are ordered and not balanced (e.g. there are munch more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods. \\\\n\\\\n5. Number of Instances: red wine - first 1599 instances; white wine - instances 1600 to 6497. \\\\n \\\\n6. Number of Attributes: 11 + output attribute\\\\nNote: several of the attributes may be correlated, thus it makes sense to apply some sort of feature selection.\\\\n\\\\n7. Attribute information:\\\\nFor more information, read [Cortez et al., 2009].\\\\nInput variables (based on physicochemical tests):\\\\n1 - fixed acidity\\\\n2 - volatile acidity\\\\n3 - citric acid\\\\n4 - residual sugar\\\\n5 - chlorides\\\\n6 - free sulfur dioxide\\\\n7 - total sulfur dioxide\\\\n8 - density\\\\n9 - pH\\\\n10 - sulphates\\\\n11 - alcohol\\\\nOutput variable (based on sensory data): \\\\n12 - quality (score between 0 and 10)\\\\n\\\\n8. Missing Attribute Values: None\\', \\'Features: \\': \\'[[0 - fixed.acidity (numeric)], [1 - volatile.acidity (numeric)], [2 - citric.acid (numeric)], [3 - residual.sugar (numeric)], [4 - chlorides (numeric)], [5 - free.sulfur.dioxide (numeric)], [6 - total.sulfur.dioxide (numeric)], [7 - density (numeric)], [8 - pH (numeric)], [9 - sulphates (numeric)], [10 - alcohol (numeric)], [11 - quality (numeric)]]\\'}\\n\\n Column of the dataset to classify: \\'quality\\': [5, 6, 4, 7]\\nDoes this column, in the context of the dataset, contain information relating to a natural person?'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prompt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch was created with id: batch_67c85a02134c81908b65d018d7e79bbd\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Status: completedg, Elapsed Time: 0 Hours 1 Minutes 20 Secondss\n",
      "Outputfile: file-3qkFRpeLjPjiTcGJcpsHBf\n",
      "\n",
      "Writing to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "resultopen2 = gpt.ask_gpt(batchfile='batchopen207.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Feature  batch_gpt_prediction\n",
      "0          Unnamed:_0                 False\n",
      "1                Date                 False\n",
      "2        AveragePrice                 False\n",
      "3        Total_Volume                 False\n",
      "4                4046                 False\n",
      "..                ...                   ...\n",
      "111           density                 False\n",
      "112                pH                 False\n",
      "113         sulphates                 False\n",
      "114           alcohol                 False\n",
      "115  {quality: false}                 False\n",
      "\n",
      "[116 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract valid entries\n",
    "result_formattedopen2 = []\n",
    "for r in resultopen2:\n",
    "    start = r.find(\"[\") + 1\n",
    "    end = r.rfind(\"]\")\n",
    "    if start > 0 and end != -1:\n",
    "        result_formattedopen2.append(r[start:end])\n",
    "    else:\n",
    "        result_formattedopen2.append(r)\n",
    "\n",
    "# Step 2: Parse and extract feature names + predictions\n",
    "parsed_resultsopen2 = []\n",
    "for r in result_formattedopen2:\n",
    "    feature, prediction = extract_dict_content(r.strip())\n",
    "    if prediction is None:\n",
    "        # If not a dictionary, try to split by comma\n",
    "        parts = r.split(',')\n",
    "        if len(parts) == 2:\n",
    "            feature, prediction = parts[0].strip(), parts[1].strip().lower() == 'true'\n",
    "        else:\n",
    "            feature, prediction = r, None\n",
    "    parsed_resultsopen2.append((feature, prediction))\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "\n",
    "df_predictionsopen2 = pd.DataFrame(parsed_resultsopen2, columns=['Feature', 'batch_gpt_prediction'])\n",
    "\n",
    "# Step 4: Clean up the DataFrame\n",
    "\n",
    "df_predictionsopen2['Feature'] = df_predictionsopen2['Feature'].str.strip()\n",
    "\n",
    "df_predictionsopen2['batch_gpt_prediction'] = df_predictionsopen2['batch_gpt_prediction'].fillna(False)\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "df_predictionsopen2.to_csv('open2_predictions_batch_gpt07.csv', index=False)\n",
    "\n",
    "# Step 6: Display DataFrame\n",
    "print(df_predictionsopen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>batch_gpt_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unnamed:_0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AveragePrice</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total_Volume</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4046</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>density</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>pH</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>sulphates</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>{quality: false}</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Feature  batch_gpt_prediction\n",
       "0          Unnamed:_0                 False\n",
       "1                Date                 False\n",
       "2        AveragePrice                 False\n",
       "3        Total_Volume                 False\n",
       "4                4046                 False\n",
       "..                ...                   ...\n",
       "111           density                 False\n",
       "112                pH                 False\n",
       "113         sulphates                 False\n",
       "114           alcohol                 False\n",
       "115  {quality: false}                 False\n",
       "\n",
       "[105 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictionsopen2[df_predictionsopen2['batch_gpt_prediction']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenML All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        0.3 temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Email Address</th>\n",
       "      <th>Username</th>\n",
       "      <th>Date of Birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Location</th>\n",
       "      <th>Membership Start Date</th>\n",
       "      <th>Membership End Date</th>\n",
       "      <th>Subscription Plan</th>\n",
       "      <th>...</th>\n",
       "      <th>citric.acid</th>\n",
       "      <th>residual.sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free.sulfur.dioxide</th>\n",
       "      <th>total.sulfur.dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ronald Murphy</td>\n",
       "      <td>williamholland@example.com</td>\n",
       "      <td>williamholland</td>\n",
       "      <td>1953-06-03</td>\n",
       "      <td>Male</td>\n",
       "      <td>Rebeccachester</td>\n",
       "      <td>2024-01-15</td>\n",
       "      <td>2025-01-14</td>\n",
       "      <td>Annual</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Scott Allen</td>\n",
       "      <td>scott22@example.org</td>\n",
       "      <td>scott22</td>\n",
       "      <td>1978-07-08</td>\n",
       "      <td>Male</td>\n",
       "      <td>Mcphersonview</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>2025-01-06</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Jonathan Parrish</td>\n",
       "      <td>brooke16@example.org</td>\n",
       "      <td>brooke16</td>\n",
       "      <td>1994-12-06</td>\n",
       "      <td>Female</td>\n",
       "      <td>Youngfort</td>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>2025-04-13</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Megan Williams</td>\n",
       "      <td>elizabeth31@example.net</td>\n",
       "      <td>elizabeth31</td>\n",
       "      <td>1964-12-22</td>\n",
       "      <td>Female</td>\n",
       "      <td>Feliciashire</td>\n",
       "      <td>2024-01-24</td>\n",
       "      <td>2025-01-23</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Kathryn Brown</td>\n",
       "      <td>pattersonalexandra@example.org</td>\n",
       "      <td>pattersonalexandra</td>\n",
       "      <td>1961-06-04</td>\n",
       "      <td>Male</td>\n",
       "      <td>Port Deborah</td>\n",
       "      <td>2024-02-14</td>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>Annual</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Mackenzie Harris</td>\n",
       "      <td>andersonemily@example.net</td>\n",
       "      <td>andersonemily</td>\n",
       "      <td>1963-11-06</td>\n",
       "      <td>Female</td>\n",
       "      <td>Lake Barryland</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>Annual</td>\n",
       "      <td>...</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.058</td>\n",
       "      <td>17.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.60</td>\n",
       "      <td>12.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Stephanie Potter</td>\n",
       "      <td>heather10@example.org</td>\n",
       "      <td>heather10</td>\n",
       "      <td>1958-05-05</td>\n",
       "      <td>Female</td>\n",
       "      <td>New Teresaburgh</td>\n",
       "      <td>2024-03-25</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.102</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.56</td>\n",
       "      <td>10.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Mitchell Potter</td>\n",
       "      <td>fsanchez@example.net</td>\n",
       "      <td>fsanchez</td>\n",
       "      <td>1979-08-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>South Katieburgh</td>\n",
       "      <td>2024-01-27</td>\n",
       "      <td>2025-01-26</td>\n",
       "      <td>Annual</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.070</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Jonathan Rodriguez</td>\n",
       "      <td>nichole85@example.net</td>\n",
       "      <td>nichole85</td>\n",
       "      <td>1967-12-20</td>\n",
       "      <td>Male</td>\n",
       "      <td>East Shannonhaven</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.079</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Jennifer Davis</td>\n",
       "      <td>zunigashelly@example.org</td>\n",
       "      <td>zunigashelly</td>\n",
       "      <td>1950-01-15</td>\n",
       "      <td>Female</td>\n",
       "      <td>West Nathanielton</td>\n",
       "      <td>2024-02-14</td>\n",
       "      <td>2025-02-13</td>\n",
       "      <td>Annual</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.080</td>\n",
       "      <td>13.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    User ID                Name                   Email Address  \\\n",
       "0         1       Ronald Murphy      williamholland@example.com   \n",
       "1         2         Scott Allen             scott22@example.org   \n",
       "2         3    Jonathan Parrish            brooke16@example.org   \n",
       "3         4      Megan Williams         elizabeth31@example.net   \n",
       "4         5       Kathryn Brown  pattersonalexandra@example.org   \n",
       "..      ...                 ...                             ...   \n",
       "95       96    Mackenzie Harris       andersonemily@example.net   \n",
       "96       97    Stephanie Potter           heather10@example.org   \n",
       "97       98     Mitchell Potter            fsanchez@example.net   \n",
       "98       99  Jonathan Rodriguez           nichole85@example.net   \n",
       "99      100      Jennifer Davis        zunigashelly@example.org   \n",
       "\n",
       "              Username Date of Birth  Gender           Location  \\\n",
       "0       williamholland    1953-06-03    Male     Rebeccachester   \n",
       "1              scott22    1978-07-08    Male      Mcphersonview   \n",
       "2             brooke16    1994-12-06  Female          Youngfort   \n",
       "3          elizabeth31    1964-12-22  Female       Feliciashire   \n",
       "4   pattersonalexandra    1961-06-04    Male       Port Deborah   \n",
       "..                 ...           ...     ...                ...   \n",
       "95       andersonemily    1963-11-06  Female     Lake Barryland   \n",
       "96           heather10    1958-05-05  Female    New Teresaburgh   \n",
       "97            fsanchez    1979-08-27    Male   South Katieburgh   \n",
       "98           nichole85    1967-12-20    Male  East Shannonhaven   \n",
       "99        zunigashelly    1950-01-15  Female  West Nathanielton   \n",
       "\n",
       "   Membership Start Date Membership End Date Subscription Plan  ...  \\\n",
       "0             2024-01-15          2025-01-14            Annual  ...   \n",
       "1             2024-01-07          2025-01-06           Monthly  ...   \n",
       "2             2024-04-13          2025-04-13           Monthly  ...   \n",
       "3             2024-01-24          2025-01-23           Monthly  ...   \n",
       "4             2024-02-14          2025-02-13            Annual  ...   \n",
       "..                   ...                 ...               ...  ...   \n",
       "95            2024-03-01          2025-03-01            Annual  ...   \n",
       "96            2024-03-25          2025-03-25           Monthly  ...   \n",
       "97            2024-01-27          2025-01-26            Annual  ...   \n",
       "98            2024-02-29          2025-02-28           Monthly  ...   \n",
       "99            2024-02-14          2025-02-13            Annual  ...   \n",
       "\n",
       "   citric.acid residual.sugar chlorides free.sulfur.dioxide  \\\n",
       "0         0.00            1.9     0.076                11.0   \n",
       "1         0.00            2.6     0.098                25.0   \n",
       "2         0.04            2.3     0.092                15.0   \n",
       "3         0.56            1.9     0.075                17.0   \n",
       "4         0.00            1.9     0.076                11.0   \n",
       "..         ...            ...       ...                 ...   \n",
       "95        0.17            2.3     0.058                17.0   \n",
       "96        0.00            3.0     0.102                 8.0   \n",
       "97        0.25            2.0     0.070                 3.0   \n",
       "98        0.06            2.5     0.079                 5.0   \n",
       "99        0.18            1.9     0.080                13.0   \n",
       "\n",
       "   total.sulfur.dioxide density    pH  sulphates  alcohol quality  \n",
       "0                  34.0  0.9978  3.51       0.56      9.4       5  \n",
       "1                  67.0  0.9968  3.20       0.68      9.8       5  \n",
       "2                  54.0  0.9970  3.26       0.65      9.8       5  \n",
       "3                  60.0  0.9980  3.16       0.58      9.8       6  \n",
       "4                  34.0  0.9978  3.51       0.56      9.4       5  \n",
       "..                  ...     ...   ...        ...      ...     ...  \n",
       "95                106.0  0.9932  3.85       0.60     12.9       6  \n",
       "96                 23.0  0.9965  3.45       0.56     10.7       5  \n",
       "97                 22.0  0.9963  3.25       0.63      9.2       5  \n",
       "98                 10.0  0.9967  3.39       0.56      9.8       5  \n",
       "99                 35.0  0.9972  3.30       0.59      9.0       6  \n",
       "\n",
       "[100 rows x 258 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openmlall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     token_count\n",
      "0         2679.0\n",
      "1         2727.0\n",
      "2         2799.0\n",
      "3         2736.0\n",
      "4         2832.0\n",
      "..           ...\n",
      "253       2688.0\n",
      "254       2664.0\n",
      "255       2670.0\n",
      "256       2664.0\n",
      "257       2547.0\n",
      "\n",
      "[258 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_token_count_openall = pd.DataFrame()\n",
    "for i in range(openmlall_data.shape[1]):\n",
    "    conversationall = [\n",
    "        {\"role\": \"system\", \"content\": initial_prompt_kaggle},\n",
    "        {\"role\": \"user\", \"content\": example_prompt_kaggle},\n",
    "        {\"role\": \"assistant\", \"content\": example_answer_kaggle}\n",
    "    ]\n",
    "    \n",
    "    with open(f\"../datasets/OpenMLall_dataset/{openmlall_datasets.iloc[i,0]}/metadata.json\", \"r\") as f:\n",
    "        metadataall = json.load(f)\n",
    "    data_promptall = \"Classify the following column with careful consideration of the dataset's description:\\n\" + str(metadataall) + \"\\n\"\n",
    "    \n",
    "    sall = \"\\n Column of the dataset to classify: '\" + openmlall_data.columns[i] + \"': \"\n",
    "    val_listall = openmlall_data.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "    sall += str(val_listall) + \"\\n\"\n",
    "\n",
    "    data_promptall = data_promptall + sall + classification_prompt_kaggle\n",
    "        \n",
    "    conversationall.append({\"role\": \"user\", \"content\": data_promptall})\n",
    "\n",
    "    for c in conversationall:\n",
    "        prompt1openall = num_tokens_from_string(c['content'], \"gpt-4o\") # system\n",
    "        prompt2openall = num_tokens_from_string(c['content'], \"gpt-4o\") # user\n",
    "        prompt3openall = num_tokens_from_string(c['content'], \"gpt-4o\") # assistant\n",
    "        df_token_count_openall.loc[i, 'token_count'] = prompt1openall + prompt2openall + prompt3openall\n",
    "\n",
    "\n",
    "    gpt.add_batch('batchopenall03.jsonl', conversationall) # Updaqte the name of the jsonl\n",
    "    \n",
    "print(df_token_count_openall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch was created with id: batch_67dc713402288190b2668c26a39ccfd4\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Status: in_progress, Elapsed Time: 0 Hours 1 Minutes 1 Secondss"
     ]
    }
   ],
   "source": [
    "resultopenall = gpt.ask_gpt(batchfile='batchopenall03.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Extract valid entries\n",
    "result_formattedopenall = []\n",
    "for r in resultopenall:\n",
    "    start = r.find(\"[\") + 1\n",
    "    end = r.rfind(\"]\")\n",
    "    if start > 0 and end != -1:\n",
    "        result_formattedopenall.append(r[start:end])\n",
    "    else:\n",
    "        result_formattedopenall.append(r)\n",
    "\n",
    "# Step 2: Parse and extract feature names + predictions\n",
    "parsed_resultsopenall = []\n",
    "for r in result_formattedopenall:\n",
    "    feature, prediction = extract_dict_content(r.strip())\n",
    "    if prediction is None:\n",
    "        # If not a dictionary, try to split by comma\n",
    "        parts = r.split(',')\n",
    "        if len(parts) == 2:\n",
    "            feature, prediction = parts[0].strip(), parts[1].strip().lower() == 'true'\n",
    "        else:\n",
    "            feature, prediction = r, None\n",
    "    parsed_resultsopenall.append((feature, prediction))\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "\n",
    "df_predictionsopenall = pd.DataFrame(parsed_resultsopenall, columns=['Feature', 'batch_gpt_prediction'])\n",
    "\n",
    "# Step 4: Clean up the DataFrame\n",
    "\n",
    "df_predictionsopenall['Feature'] = df_predictionsopenall['Feature'].str.strip()\n",
    "\n",
    "df_predictionsopenall['batch_gpt_prediction'] = df_predictionsopenall['batch_gpt_prediction'].fillna(False)\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "df_predictionsopenall.to_csv('openall_predictions_batch_gpt03.csv', index=False)\n",
    "\n",
    "# Step 6: Display DataFrame\n",
    "print(df_predictionsopenall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on medical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    token_count\n",
      "0         615.0\n",
      "1         549.0\n",
      "2         231.0\n",
      "3         360.0\n",
      "4         237.0\n",
      "5         639.0\n",
      "6         234.0\n",
      "7         234.0\n",
      "8         234.0\n",
      "9         231.0\n",
      "10        234.0\n",
      "11        234.0\n",
      "12        243.0\n",
      "13        240.0\n",
      "14        435.0\n",
      "15        381.0\n",
      "16        339.0\n",
      "17        243.0\n",
      "18        237.0\n",
      "19        246.0\n",
      "20        252.0\n",
      "21        252.0\n",
      "22        483.0\n",
      "23        483.0\n",
      "24        333.0\n",
      "25        309.0\n",
      "26        315.0\n",
      "27        228.0\n",
      "28        231.0\n",
      "29        231.0\n",
      "30        228.0\n",
      "31        231.0\n"
     ]
    }
   ],
   "source": [
    "df_token_count_med = pd.DataFrame()\n",
    "for i in range(medical_data.shape[1]):\n",
    "    conversation5 = [\n",
    "        {\"role\": \"system\", \"content\": initial_prompt_kaggle},\n",
    "        {\"role\": \"user\", \"content\": example_prompt_kaggle},\n",
    "        {\"role\": \"assistant\", \"content\": example_answer_kaggle}\n",
    "    ]\n",
    "    \n",
    "    metadata5 = {\n",
    "    \"Dataset Name\": \"Medical Patient Dataset\",\n",
    "    \"Description\": \"\"\"  This dataset contains personal information about patients in a medical facility.\n",
    "    \"\"\"\n",
    "    }\n",
    "    data_prompt5 = \"Classify the following column with careful consideration of the dataset description:\\n\" + str(metadata5) + \"\\n\"\n",
    "\n",
    "    s5 = \"\\n Column of the dataset to classify: '\" + medical_data.columns[i] + \"': \"\n",
    "    val_list5 = medical_data.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "    s5 += str(val_list5) + \"\\n\"\n",
    "\n",
    "    data_prompt5 = data_prompt5 + s5 + classification_prompt_kaggle\n",
    "        \n",
    "    conversation5.append({\"role\": \"user\", \"content\": data_prompt5})\n",
    "\n",
    "    for c in conversation5:\n",
    "        prompt1med = num_tokens_from_string(c['content'], \"gpt-4o\") # system\n",
    "        prompt2med = num_tokens_from_string(c['content'], \"gpt-4o\") # user\n",
    "        prompt3med = num_tokens_from_string(c['content'], \"gpt-4o\") # assistant\n",
    "        df_token_count_med.loc[i, 'token_count'] = prompt1med + prompt2med + prompt3med\n",
    "\n",
    "\n",
    "    gpt.add_batch('batchmed07.jsonl', conversation5) # Updaqte the name of the jsonl\n",
    "    \n",
    "    # response = openai.chat.completions.create(\n",
    "    #     model=\"gpt-4o\",\n",
    "    #     messages=conversation,\n",
    "    #     seed=42\n",
    "    # )\n",
    "    \n",
    "    # with open(\"gpt_predictions/medical_results.txt\", \"a\") as file:\n",
    "    #     file.write(\"\\n\" + response.choices[0].message.content.replace(\"\\n\", \"\"))\n",
    "print(df_token_count_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Classify the following column with careful consideration of the dataset description:\\n{'Dataset Name': 'Medical Patient Dataset', 'Description': '  This dataset contains personal information about patients in a medical facility.\\\\n    '}\\n\\n Column of the dataset to classify: 'amphetamin': [0]\\nDoes this column, in the context of the dataset, contain information relating to a natural person?\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prompt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "95    0\n",
       "96    0\n",
       "97    0\n",
       "98    0\n",
       "99    0\n",
       "Name: amphetamin, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical_data['amphetamin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch was created with id: batch_67c85a69a27c8190bb459bc16a13dcda\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Status: completedss, Elapsed Time: 0 Hours 1 Minutes 41 Seconds\n",
      "Outputfile: file-6FpygQxS7w6YXoSmifmiGu\n",
      "\n",
      "Writing to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "resultmed = gpt.ask_gpt(batchfile='batchmed07.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_formattedmed = []\n",
    "\n",
    "# for r in resultmed:\n",
    "#     start = r.index(\"[\") + 1\n",
    "#     end = r.index(\"]\")\n",
    "#     # Extract the substring\n",
    "#     res5 = r[start:end]\n",
    "#     res5= res5.replace(\"'\", '')\n",
    "#     res5 = res5.replace('\"', '')\n",
    "#     # if res == None:\n",
    "#     #     res = ''\n",
    "#     result_formattedmed.append(res5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encounter_id',\n",
       " 'patient_id',\n",
       " 'admission_type',\n",
       " 'admission_reason',\n",
       " 'last_encounter',\n",
       " 'birth_date',\n",
       " 'aids',\n",
       " 'wloss',\n",
       " 'alcohol',\n",
       " 'drug',\n",
       " 'psycho',\n",
       " 'depre',\n",
       " 'hemorhage_stroke',\n",
       " 'ischemic_stroke',\n",
       " 'first_operation_code',\n",
       " 'major_surgery',\n",
       " 'discharge',\n",
       " 'gender',\n",
       " 'dementia',\n",
       " 'any_medication',\n",
       " 'any_grouped_medication',\n",
       " 'any_acute_medication',\n",
       " 'age_at_visit',\n",
       " 'total_number_operations',\n",
       " 'total_opeartion_duration',\n",
       " 'number_of_surgeries',\n",
       " 'major_surgery_duration',\n",
       " 'cocaine',\n",
       " 'cannabis',\n",
       " 'barbiturate',\n",
       " 'opiate',\n",
       " 'amphetamin']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_formattedmm= []\n",
    "# for r in resultm:\n",
    "#     start = r.find(\"[\") + 1\n",
    "#     end = r.find(\"]\")\n",
    "\n",
    "#     if start == 0 or end == -1:  # Meaning \"[\" or \"]\" is missing\n",
    "#         print(f\"{r}\")\n",
    "#         result_formattedkaggle1.append(r) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"{'encounter_id': true}\",\n",
       " \"{'patient_id': true}\",\n",
       " \"{'admission_type': true}\",\n",
       " \"{'admission_reason': true}\",\n",
       " \"{'last_encounter': true}\",\n",
       " \"{'birth_date': true}\",\n",
       " \"{'aids': true}\",\n",
       " \"{'wloss': true}\",\n",
       " \"{'alcohol': true}\",\n",
       " \"{'drug': True}\",\n",
       " \"{'psycho': true}\",\n",
       " \"{'depre': true}\",\n",
       " \"{'hemorhage_stroke': true}\",\n",
       " \"{'ischemic_stroke': true}\",\n",
       " \"{'first_operation_code': true}\",\n",
       " \"{'major_surgery': true}\",\n",
       " \"{'discharge': true}\",\n",
       " \"{'gender': true}\",\n",
       " \"{'dementia': true}\",\n",
       " \"{'any_medication': true}\",\n",
       " \"{'any_grouped_medication': true}\",\n",
       " \"{'any_acute_medication': true}\",\n",
       " \"{'age_at_visit': true}\",\n",
       " \"{'total_number_operations': true}\",\n",
       " \"{'total_opeartion_duration': true}\",\n",
       " \"{'number_of_surgeries': true}\",\n",
       " \"{'major_surgery_duration': true}\",\n",
       " \"{'cocaine': true}\",\n",
       " \"{'cannabis': true}\",\n",
       " \"{'barbiturate': true}\",\n",
       " \"{'opiate': true}\",\n",
       " \"{'amphetamin': true}\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def extract_dict_content(s):\n",
    "    try:\n",
    "        # Try to parse as a dictionary\n",
    "        d = ast.literal_eval(s)\n",
    "        if isinstance(d, dict):\n",
    "            return next(iter(d.items()))\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If parsing fails, try to extract using regex\n",
    "        match = re.match(r\"{'(.+?)'\\s*:\\s*(true|false)}\", s, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1), match.group(2).lower() == 'true'\n",
    "    return s, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Feature  batch_gpt_prediction\n",
      "0               encounter_id                  True\n",
      "1                 patient_id                  True\n",
      "2             admission_type                  True\n",
      "3           admission_reason                  True\n",
      "4             last_encounter                  True\n",
      "5                 birth_date                  True\n",
      "6                       aids                  True\n",
      "7                      wloss                  True\n",
      "8                    alcohol                  True\n",
      "9                       drug                  True\n",
      "10                    psycho                  True\n",
      "11                     depre                  True\n",
      "12          hemorhage_stroke                  True\n",
      "13           ischemic_stroke                  True\n",
      "14      first_operation_code                  True\n",
      "15             major_surgery                  True\n",
      "16                 discharge                  True\n",
      "17                    gender                  True\n",
      "18                  dementia                  True\n",
      "19            any_medication                  True\n",
      "20    any_grouped_medication                  True\n",
      "21      any_acute_medication                  True\n",
      "22              age_at_visit                  True\n",
      "23   total_number_operations                  True\n",
      "24  total_opeartion_duration                  True\n",
      "25       number_of_surgeries                  True\n",
      "26    major_surgery_duration                  True\n",
      "27                   cocaine                  True\n",
      "28                  cannabis                  True\n",
      "29               barbiturate                  True\n",
      "30                    opiate                  True\n",
      "31                amphetamin                  True\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract valid entries\n",
    "result_formattedmed = []\n",
    "for r in resultmed:\n",
    "    start = r.find(\"[\") + 1\n",
    "    end = r.rfind(\"]\")\n",
    "    if start > 0 and end != -1:\n",
    "        result_formattedmed.append(r[start:end])\n",
    "    else:\n",
    "        result_formattedmed.append(r)\n",
    "\n",
    "# Step 2: Parse and extract feature names + predictions\n",
    "parsed_resultsmed = []\n",
    "for r in result_formattedmed:\n",
    "    feature, prediction = extract_dict_content(r.strip())\n",
    "    if prediction is None:\n",
    "        # If not a dictionary, try to split by comma\n",
    "        parts = r.split(',')\n",
    "        if len(parts) == 2:\n",
    "            feature, prediction = parts[0].strip(), parts[1].strip().lower() == 'true'\n",
    "        else:\n",
    "            feature, prediction = r, None\n",
    "    parsed_resultsmed.append((feature, prediction))\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "\n",
    "df_predictionsmed = pd.DataFrame(parsed_resultsmed, columns=['Feature', 'batch_gpt_prediction'])\n",
    "\n",
    "# Step 4: Clean up the DataFrame\n",
    "\n",
    "df_predictionsmed['Feature'] = df_predictionsmed['Feature'].str.strip()\n",
    "\n",
    "df_predictionsmed['batch_gpt_prediction'] = df_predictionsmed['batch_gpt_prediction'].fillna(False)\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "df_predictionsmed.to_csv('med_predictions_batch_gpt07.csv', index=False)\n",
    "\n",
    "# Step 6: Display DataFrame\n",
    "print(df_predictionsmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the real MedHonda Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIMIC DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIMICmedical_data = pd.read_csv('/home/aagisha/Documents/PhD Albert/CodingPhDStart1/Bachelor Projects HSAA/PIIDETECTION/Personal-Detection/datasets/MIMICIIIandHonda/Mimic_GPT_Evaluation.csv')\n",
    "# MIMICmedical_labels = pd.read_csv('/home/aagisha/Documents/PhD Albert/CodingPhDStart1/Bachelor Projects HSAA/PIIDETECTION/Personal-Detection/datasets/MIMICIIIandHonda/column_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colum = MIMICmedical_data.columns.to_list()[1:]\n",
    "# MIMICmedical= MIMICmedical_data[colum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMICmedical_Used= pd.read_csv('../datasets/MIMICIIIandHonda/Mimic_for_Use.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 164)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIMICmedical_Used.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum1 = MIMICmedical_Used.columns.to_list()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMICmedical_Used= MIMICmedical_Used[colum1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>dbsource</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>prev_careunit</th>\n",
       "      <th>curr_careunit</th>\n",
       "      <th>prev_wardid</th>\n",
       "      <th>curr_wardid</th>\n",
       "      <th>...</th>\n",
       "      <th>location</th>\n",
       "      <th>locationcategory</th>\n",
       "      <th>first_careunit</th>\n",
       "      <th>last_careunit</th>\n",
       "      <th>first_wardid</th>\n",
       "      <th>last_wardid</th>\n",
       "      <th>drg_type</th>\n",
       "      <th>drg_code</th>\n",
       "      <th>drg_severity</th>\n",
       "      <th>drg_mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54440</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>142345.0</td>\n",
       "      <td>206504.0</td>\n",
       "      <td>carevue</td>\n",
       "      <td>admit</td>\n",
       "      <td></td>\n",
       "      <td>MICU</td>\n",
       "      <td></td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54441</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>142345.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>transfer</td>\n",
       "      <td>MICU</td>\n",
       "      <td></td>\n",
       "      <td>52.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54442</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>142345.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>discharge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>45.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54460</td>\n",
       "      <td>10011.0</td>\n",
       "      <td>105331.0</td>\n",
       "      <td>232110.0</td>\n",
       "      <td>carevue</td>\n",
       "      <td>admit</td>\n",
       "      <td></td>\n",
       "      <td>MICU</td>\n",
       "      <td></td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54461</td>\n",
       "      <td>10011.0</td>\n",
       "      <td>105331.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>discharge</td>\n",
       "      <td>MICU</td>\n",
       "      <td></td>\n",
       "      <td>15.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>54915</td>\n",
       "      <td>10119.0</td>\n",
       "      <td>157466.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>admit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>54916</td>\n",
       "      <td>10119.0</td>\n",
       "      <td>157466.0</td>\n",
       "      <td>247686.0</td>\n",
       "      <td>carevue</td>\n",
       "      <td>transfer</td>\n",
       "      <td></td>\n",
       "      <td>SICU</td>\n",
       "      <td>36.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>54917</td>\n",
       "      <td>10119.0</td>\n",
       "      <td>157466.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>transfer</td>\n",
       "      <td>SICU</td>\n",
       "      <td></td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>54918</td>\n",
       "      <td>10119.0</td>\n",
       "      <td>157466.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>transfer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>54919</td>\n",
       "      <td>10119.0</td>\n",
       "      <td>157466.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>transfer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_id  subject_id   hadm_id icustay_id dbsource  eventtype  \\\n",
       "0     54440     10006.0  142345.0   206504.0  carevue      admit   \n",
       "1     54441     10006.0  142345.0             carevue   transfer   \n",
       "2     54442     10006.0  142345.0             carevue  discharge   \n",
       "3     54460     10011.0  105331.0   232110.0  carevue      admit   \n",
       "4     54461     10011.0  105331.0             carevue  discharge   \n",
       "..      ...         ...       ...        ...      ...        ...   \n",
       "195   54915     10119.0  157466.0             carevue      admit   \n",
       "196   54916     10119.0  157466.0   247686.0  carevue   transfer   \n",
       "197   54917     10119.0  157466.0             carevue   transfer   \n",
       "198   54918     10119.0  157466.0             carevue   transfer   \n",
       "199   54919     10119.0  157466.0             carevue   transfer   \n",
       "\n",
       "    prev_careunit curr_careunit prev_wardid curr_wardid  ... location  \\\n",
       "0                          MICU                    52.0  ...            \n",
       "1            MICU                      52.0        45.0  ...            \n",
       "2                                      45.0              ...            \n",
       "3                          MICU                    15.0  ...            \n",
       "4            MICU                      15.0              ...            \n",
       "..            ...           ...         ...         ...  ...      ...   \n",
       "195                                                36.0  ...            \n",
       "196                        SICU        36.0        33.0  ...            \n",
       "197          SICU                      33.0         3.0  ...            \n",
       "198                                     3.0         3.0  ...            \n",
       "199                                     3.0         3.0  ...            \n",
       "\n",
       "    locationcategory first_careunit last_careunit first_wardid last_wardid  \\\n",
       "0                                                                            \n",
       "1                                                                            \n",
       "2                                                                            \n",
       "3                                                                            \n",
       "4                                                                            \n",
       "..               ...            ...           ...          ...         ...   \n",
       "195                                                                          \n",
       "196                                                                          \n",
       "197                                                                          \n",
       "198                                                                          \n",
       "199                                                                          \n",
       "\n",
       "    drg_type drg_code drg_severity drg_mortality  \n",
       "0                                                 \n",
       "1                                                 \n",
       "2                                                 \n",
       "3                                                 \n",
       "4                                                 \n",
       "..       ...      ...          ...           ...  \n",
       "195                                               \n",
       "196                                               \n",
       "197                                               \n",
       "198                                               \n",
       "199                                               \n",
       "\n",
       "[200 rows x 163 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIMICmedical_Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_prompt_mimic = \"Does this column contain information that, alone or in combination with one or more other columns, could contribute to identifying a natural person in the context of the MIMIC-III dataset?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     token_count\n",
      "0          648.0\n",
      "1          708.0\n",
      "2          711.0\n",
      "3          702.0\n",
      "4          540.0\n",
      "..           ...\n",
      "158        543.0\n",
      "159        540.0\n",
      "160        540.0\n",
      "161        543.0\n",
      "162        546.0\n",
      "\n",
      "[163 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_token_count_mimic1 = pd.DataFrame()\n",
    "for i in range(MIMICmedical_Used.shape[1]):\n",
    "    conversation7 = [\n",
    "        {\"role\": \"system\", \"content\": initial_prompt_kaggle},\n",
    "        {\"role\": \"user\", \"content\": example_prompt_kaggle},\n",
    "        {\"role\": \"assistant\", \"content\": example_answer_kaggle}\n",
    "    ]\n",
    "    \n",
    "    metadata7 = {\n",
    "    \"Dataset Name\": \"Compiled MIMIC-III Dataset\",\n",
    "    \"Description\": \"\"\"This dataset is a compiled version of MIMIC-III (Medical Information Mart for Intensive Care III), containing structured medical and administrative records of ICU patients from Beth Israel Deaconess Medical Center between 2001 and 2012. \n",
    "    It includes patient demographics, hospital admissions, clinical events, vital signs, laboratory results, medications, caregiver notes, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and billing information. \"\"\"\n",
    "}\n",
    "\n",
    "    data_prompt7 = \"Classify the following column with careful consideration of the dataset description:\\n\" + str(metadata7) + \"\\n\"\n",
    "\n",
    "    s7 = \"\\n Column of the dataset to classify: '\" + MIMICmedical_Used.columns[i] + \"': \"\n",
    "    val_list7 = MIMICmedical_Used.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "    s7 += str(val_list7) + \"\\n\"\n",
    "\n",
    "    data_prompt7 = data_prompt7 + s7 + classification_prompt_mimic\n",
    "        \n",
    "    conversation7.append({\"role\": \"user\", \"content\": data_prompt7})\n",
    "\n",
    "    for c in conversation7:\n",
    "        prompt1m7 = num_tokens_from_string(c['content'], \"gpt-4o\") # system\n",
    "        prompt2m7 = num_tokens_from_string(c['content'], \"gpt-4o\") # user\n",
    "        prompt3m7 = num_tokens_from_string(c['content'], \"gpt-4o\") # assistant\n",
    "        df_token_count_mimic1.loc[i, 'token_count'] = prompt1m7 + prompt2m7 + prompt3m7\n",
    "\n",
    "\n",
    "    gpt.add_batch('batchmimic01New.jsonl', conversation7) # Updaqte the name of the jsonl\n",
    "    \n",
    "    # response = openai.chat.completions.create(\n",
    "    #     model=\"gpt-4o\",\n",
    "    #     messages=conversation,\n",
    "    #     seed=42\n",
    "    # )\n",
    "    \n",
    "    # with open(\"gpt_predictions/medical_results.txt\", \"a\") as file:\n",
    "    #     file.write(\"\\n\" + response.choices[0].message.content.replace(\"\\n\", \"\"))\n",
    "print(df_token_count_mimic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Classify the following column with careful consideration of the dataset description:\\n{'Dataset Name': 'Compiled MIMIC-III Dataset', 'Description': 'This dataset is a compiled version of MIMIC-III (Medical Information Mart for Intensive Care III), containing structured medical and administrative records of ICU patients from Beth Israel Deaconess Medical Center between 2001 and 2012. \\\\n    It includes patient demographics, hospital admissions, clinical events, vital signs, laboratory results, medications, caregiver notes, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and billing information. '}\\n\\n Column of the dataset to classify: 'drg_mortality': [' ']\\nDoes this column contain information that, alone or in combination with one or more other columns, could contribute to identifying a natural person in the context of the MIMIC-III dataset?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prompt7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>dbsource</th>\n",
       "      <th>eventtype</th>\n",
       "      <th>prev_careunit</th>\n",
       "      <th>curr_careunit</th>\n",
       "      <th>prev_wardid</th>\n",
       "      <th>curr_wardid</th>\n",
       "      <th>...</th>\n",
       "      <th>location</th>\n",
       "      <th>locationcategory</th>\n",
       "      <th>first_careunit</th>\n",
       "      <th>last_careunit</th>\n",
       "      <th>first_wardid</th>\n",
       "      <th>last_wardid</th>\n",
       "      <th>drg_type</th>\n",
       "      <th>drg_code</th>\n",
       "      <th>drg_severity</th>\n",
       "      <th>drg_mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54440</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>142345.0</td>\n",
       "      <td>206504.0</td>\n",
       "      <td>carevue</td>\n",
       "      <td>admit</td>\n",
       "      <td></td>\n",
       "      <td>MICU</td>\n",
       "      <td></td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54441</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>142345.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>transfer</td>\n",
       "      <td>MICU</td>\n",
       "      <td></td>\n",
       "      <td>52.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54442</td>\n",
       "      <td>10006.0</td>\n",
       "      <td>142345.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>discharge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>45.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54460</td>\n",
       "      <td>10011.0</td>\n",
       "      <td>105331.0</td>\n",
       "      <td>232110.0</td>\n",
       "      <td>carevue</td>\n",
       "      <td>admit</td>\n",
       "      <td></td>\n",
       "      <td>MICU</td>\n",
       "      <td></td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54461</td>\n",
       "      <td>10011.0</td>\n",
       "      <td>105331.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>discharge</td>\n",
       "      <td>MICU</td>\n",
       "      <td></td>\n",
       "      <td>15.0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>54915</td>\n",
       "      <td>10119.0</td>\n",
       "      <td>157466.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>admit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>54916</td>\n",
       "      <td>10119.0</td>\n",
       "      <td>157466.0</td>\n",
       "      <td>247686.0</td>\n",
       "      <td>carevue</td>\n",
       "      <td>transfer</td>\n",
       "      <td></td>\n",
       "      <td>SICU</td>\n",
       "      <td>36.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>54917</td>\n",
       "      <td>10119.0</td>\n",
       "      <td>157466.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>transfer</td>\n",
       "      <td>SICU</td>\n",
       "      <td></td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>54918</td>\n",
       "      <td>10119.0</td>\n",
       "      <td>157466.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>transfer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>54919</td>\n",
       "      <td>10119.0</td>\n",
       "      <td>157466.0</td>\n",
       "      <td></td>\n",
       "      <td>carevue</td>\n",
       "      <td>transfer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_id  subject_id   hadm_id icustay_id dbsource  eventtype  \\\n",
       "0     54440     10006.0  142345.0   206504.0  carevue      admit   \n",
       "1     54441     10006.0  142345.0             carevue   transfer   \n",
       "2     54442     10006.0  142345.0             carevue  discharge   \n",
       "3     54460     10011.0  105331.0   232110.0  carevue      admit   \n",
       "4     54461     10011.0  105331.0             carevue  discharge   \n",
       "..      ...         ...       ...        ...      ...        ...   \n",
       "195   54915     10119.0  157466.0             carevue      admit   \n",
       "196   54916     10119.0  157466.0   247686.0  carevue   transfer   \n",
       "197   54917     10119.0  157466.0             carevue   transfer   \n",
       "198   54918     10119.0  157466.0             carevue   transfer   \n",
       "199   54919     10119.0  157466.0             carevue   transfer   \n",
       "\n",
       "    prev_careunit curr_careunit prev_wardid curr_wardid  ... location  \\\n",
       "0                          MICU                    52.0  ...            \n",
       "1            MICU                      52.0        45.0  ...            \n",
       "2                                      45.0              ...            \n",
       "3                          MICU                    15.0  ...            \n",
       "4            MICU                      15.0              ...            \n",
       "..            ...           ...         ...         ...  ...      ...   \n",
       "195                                                36.0  ...            \n",
       "196                        SICU        36.0        33.0  ...            \n",
       "197          SICU                      33.0         3.0  ...            \n",
       "198                                     3.0         3.0  ...            \n",
       "199                                     3.0         3.0  ...            \n",
       "\n",
       "    locationcategory first_careunit last_careunit first_wardid last_wardid  \\\n",
       "0                                                                            \n",
       "1                                                                            \n",
       "2                                                                            \n",
       "3                                                                            \n",
       "4                                                                            \n",
       "..               ...            ...           ...          ...         ...   \n",
       "195                                                                          \n",
       "196                                                                          \n",
       "197                                                                          \n",
       "198                                                                          \n",
       "199                                                                          \n",
       "\n",
       "    drg_type drg_code drg_severity drg_mortality  \n",
       "0                                                 \n",
       "1                                                 \n",
       "2                                                 \n",
       "3                                                 \n",
       "4                                                 \n",
       "..       ...      ...          ...           ...  \n",
       "195                                               \n",
       "196                                               \n",
       "197                                               \n",
       "198                                               \n",
       "199                                               \n",
       "\n",
       "[200 rows x 163 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIMICmedical_Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch was created with id: batch_67cb042cc11c8190bba4d12e4e540e7e\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "Status: completedss, Elapsed Time: 0 Hours 2 Minutes 22 Seconds\n",
      "Outputfile: file-9uzXivboMQQTdKggzm8d2S\n",
      "\n",
      "Writing to file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "resultmimic1 = gpt.ask_gpt(batchfile='batchmimic01New.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def extract_dict_content(s):\n",
    "    try:\n",
    "        # Try to parse as a dictionary\n",
    "        d = ast.literal_eval(s)\n",
    "        if isinstance(d, dict):\n",
    "            return next(iter(d.items()))\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If parsing fails, try to extract using regex\n",
    "        match = re.match(r\"{'(.+?)'\\s*:\\s*(true|false)}\", s, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1), match.group(2).lower() == 'true'\n",
    "    return s, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Feature batch_gpt_prediction\n",
      "0           row_id                False\n",
      "1       subject_id                 True\n",
      "2          hadm_id                 True\n",
      "3       icustay_id                 True\n",
      "4         dbsource                False\n",
      "..             ...                  ...\n",
      "158    last_wardid                 True\n",
      "159       drg_type                False\n",
      "160       drg_code                 True\n",
      "161   drg_severity                False\n",
      "162  drg_mortality                False\n",
      "\n",
      "[163 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract valid entries\n",
    "result_formattedmimic1 = []\n",
    "for r in resultmimic1:\n",
    "    start = r.find(\"[\") + 1\n",
    "    end = r.rfind(\"]\")\n",
    "    if start > 0 and end != -1:\n",
    "        result_formattedmimic1.append(r[start:end])\n",
    "    else:\n",
    "        result_formattedmimic1.append(r)\n",
    "\n",
    "# Step 2: Parse and extract feature names + predictions\n",
    "parsed_resultsmimic1 = []\n",
    "for r in result_formattedmimic1:\n",
    "    feature, prediction = extract_dict_content(r.strip())\n",
    "    if prediction is None:\n",
    "        # If not a dictionary, try to split by comma\n",
    "        parts = r.split(',')\n",
    "        if len(parts) == 2:\n",
    "            feature, prediction = parts[0].strip(), parts[1].strip().lower() == 'true'\n",
    "        else:\n",
    "            feature, prediction = r, None\n",
    "    parsed_resultsmimic1.append((feature, prediction))\n",
    "\n",
    "# Step 3: Convert to DataFrame\n",
    "\n",
    "df_predictionsmimic1 = pd.DataFrame(parsed_resultsmimic1, columns=['Feature', 'batch_gpt_prediction'])\n",
    "\n",
    "# Step 4: Clean up the DataFrame\n",
    "\n",
    "df_predictionsmimic1['Feature'] = df_predictionsmimic1['Feature'].str.strip()\n",
    "\n",
    "#df_predictionsmimic1['batch_gpt_prediction'] = df_predictionsmimic1['batch_gpt_prediction'].fillna(False)\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "df_predictionsmimic1.to_csv('mimic_predictions_batch_gpt01New.csv', index=False)\n",
    "\n",
    "# Step 6: Display DataFrame\n",
    "print(df_predictionsmimic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>batch_gpt_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>'ab_name': False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>{subsectionheader: false}</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>'abbreviation': False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature batch_gpt_prediction\n",
       "27           'ab_name': False                 None\n",
       "90  {subsectionheader: false}                 None\n",
       "91      'abbreviation': False                 None"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictionsmimic1[df_predictionsmimic1['batch_gpt_prediction'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on medical data + Honda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on kaggle data and openml data PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_prompt_kaggle_pii = \"\"\"\n",
    "# As a classifier of personal identifiable information (pii) in tabular datasets, \n",
    "# your task is to analyze the provided columns (each containing up to ten distinct values)\n",
    "# and determine whether they contain information that can directly identify a person. \n",
    "# Detecting pii data helps ensure compliance with data protection regulations and safeguards individuals' privacy and security.\n",
    "# Output your results in a dictionary format with a boolean indicating if the column contains pii data or not.\n",
    "# \"\"\"\n",
    "\n",
    "# example_prompt_kaggle_pii = \"\"\"\n",
    "# You can use the following example as a guideline:\n",
    "# Classify the following column with careful consideration of the dataset description:\n",
    "# Dataset: Title: \"Test Dataset\"\n",
    "# Description: \"This dataset was used for a linear regression.\"\n",
    "# Features: ['first_name_en_10', 'last_name_en_10', 'email_en_10', 'phone_number', 'address_en_10', 'city_en_10', 'country_en_10', 'date', 'target']\n",
    "# Column of the dataset to classify: 'first_name_en_10': ['Tom', 'Walter', 'Mia', 'Lena', 'John', 'Jack', 'Felice', 'Anna', 'Lukas', 'Will']\n",
    "# Does this column, in the context of the dataset, contain personal identifiable information?\n",
    "# \"\"\"\n",
    "\n",
    "# example_answer_kaggle_pii = \"Example Answer: {first_name_en_10: true}\"\n",
    "# classification_prompt_kaggle_pii = \"Does this column, in the context of the dataset, contain personal identifiable information?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(kaggle_data.shape[1]):\n",
    "#     conversation = [\n",
    "#         {\"role\": \"system\", \"content\": initial_prompt_kaggle_pii},\n",
    "#         {\"role\": \"user\", \"content\": example_prompt_kaggle_pii},\n",
    "#         {\"role\": \"assistant\", \"content\": example_answer_kaggle_pii}\n",
    "#     ]\n",
    "    \n",
    "#     with open(f\"../datasets/kaggle_datasets/{kaggle_dataset.iloc[i,0]}/dataset-metadata.json\", \"r\") as f:\n",
    "#         metadata = json.load(f)\n",
    "#     data_prompt = \"Classify the following column with careful consideration of the dataset description. Dataset: Title: \" + metadata[\"title\"] + \"\\n\"\n",
    "#     data_prompt = data_prompt + \"Description: \" + metadata[\"description\"] + \"\\n\"\n",
    "#     data_prompt = data_prompt + \"Features: \" + str(kaggle_data.iloc[:, kaggle_dataset.loc[kaggle_dataset[\"dataset\"] == kaggle_dataset.iloc[i,0]].index].columns)\n",
    "        \n",
    "#     s = \"\\n Column of the dataset to classify: '\" + kaggle_data.columns[i] + \"': \"\n",
    "#     val_list = kaggle_data.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "#     s += str(val_list) + \"\\n\"\n",
    "\n",
    "#     data_prompt = data_prompt + s + classification_prompt_kaggle_pii\n",
    "        \n",
    "#     conversation.append({\"role\": \"user\", \"content\": data_prompt})\n",
    "#         file.write(\"\\n\" + response.choices[0].message.content.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(openml_data.shape[1]):\n",
    "#     conversation = [\n",
    "#         {\"role\": \"system\", \"content\": initial_prompt_kaggle_pii},\n",
    "#         {\"role\": \"user\", \"content\": example_prompt_kaggle_pii},\n",
    "#         {\"role\": \"assistant\", \"content\": example_answer_kaggle_pii}\n",
    "#     ]\n",
    "    \n",
    "#     with open(f\"../datasets/openml_datasets/{openml_dataset.iloc[i,0]}/metadata.json\", \"r\") as f:\n",
    "#         metadata = json.load(f)\n",
    "#     data_prompt = \"Classify the following column with careful consideration of the dataset's description:\\n\" + str(metadata) + \"\\n\"\n",
    "    \n",
    "#     s = \"\\n Column of the dataset to classify: '\" + openml_data.columns[i] + \"': \"\n",
    "#     val_list = openml_data.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "#     s += str(val_list) + \"\\n\"\n",
    "\n",
    "#     data_prompt = data_prompt + s + classification_prompt_kaggle_pii\n",
    "        \n",
    "#     conversation.append({\"role\": \"user\", \"content\": data_prompt})\n",
    "    \n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o\",\n",
    "#         messages=conversation,\n",
    "#         seed=42\n",
    "#     )\n",
    "    \n",
    "#     with open(\"gpt_predictions/openml_results_pii.txt\", \"a\") as file:\n",
    "#         file.write(\"\\n\" + response.choices[0].message.content.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(openml_2_data.shape[1]):\n",
    "#     conversation = [\n",
    "#         {\"role\": \"system\", \"content\": initial_prompt_kaggle_pii},\n",
    "#         {\"role\": \"user\", \"content\": example_prompt_kaggle_pii},\n",
    "#         {\"role\": \"assistant\", \"content\": example_answer_kaggle_pii}\n",
    "#     ]\n",
    "    \n",
    "#     with open(f\"../datasets/openml_datasets_2/{openml_2_dataset.iloc[i,0]}/metadata.json\", \"r\") as f:\n",
    "#         metadata = json.load(f)\n",
    "#     data_prompt = \"Classify the following column with careful consideration of the dataset's description:\\n\" + str(metadata) + \"\\n\"\n",
    "\n",
    "#     s = \"\\n Column of the dataset to classify: '\" + openml_2_data.columns[i] + \"': \"\n",
    "#     val_list = openml_2_data.iloc[:, i].value_counts().index.tolist()[:10]     #value_counts sorts the values in descending order\n",
    "#     s += str(val_list) + \"\\n\"\n",
    "\n",
    "#     data_prompt = data_prompt + s + classification_prompt_kaggle_pii\n",
    "        \n",
    "#     conversation.append({\"role\": \"user\", \"content\": data_prompt})\n",
    "    \n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o\",\n",
    "#         messages=conversation,\n",
    "#         seed=42\n",
    "#     )\n",
    "    \n",
    "#     with open(\"gpt_predictions/openml_2_results_pii.txt\", \"a\") as file:\n",
    "#         file.write(\"\\n\" + response.choices[0].message.content.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
